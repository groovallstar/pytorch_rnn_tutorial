{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토큰화  \n",
    "- 주어진 텍스트를 단어 또는 문자 단위로 자르는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_text = \"A Dog Run back corner near spare bedrooms\"\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'Dog', 'Run', 'back', 'corner', 'near', 'spare', 'bedrooms']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(en_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'Dog', 'Run', 'back', 'corner', 'near', 'spare', 'bedrooms']\n"
     ]
    }
   ],
   "source": [
    "# 한국어는 단순 띄어쓰기 단위로 나누면 같은 단어가 다른 단어로 인식되어 단어 집합 크기가\n",
    "# 불필요하게 커짐\n",
    "# 단어집합 : 중복을 제거한 텍스트의 총 단어의 집합(set)\n",
    "print(en_text.split())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토치텍스트(Torchtext)  \n",
    "텍스트에 대한 여러 추상화 기능을 제공하는 자연어 처리 라이브러리  \n",
    "\n",
    "제공 기능  \n",
    "- 파일 로드하기(File Loading) : 다양한 포맷의 코퍼스 로드\n",
    "- 토큰화(Tokenization) : 문장을 단어 단위로 분리\n",
    "- 단어 집합(Vocab) : 단어 집합 생성\n",
    "- 정수 인코딩(Integer encoding) : 전체 코퍼스의 단어들을 각각의 고유한 정수로 맵핑\n",
    "- 단어 벡터(Word Vector) : 단어 집합의 단어들에 고유한 임베딩 벡터를 만들어줌. 랜덤값으로 초기화한 값일 수도 있고, 사전 훈련된 임베딩 벡터들을 로드할 수 있음\n",
    "- 배치화(Batching) : 훈련 샘플들의 배치를 만들어줌. 이 과정에서 패딩 작업(Padding)도 이뤄짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('IMDb_Reviews.csv', <http.client.HTTPMessage at 0x7fedc0316640>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "import pandas as pd\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv\",\n",
    "    filename=\"IMDb_Reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My family and I normally do not watch local mo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Believe it or not, this was at one time the wo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>After some internet surfing, I found the \"Home...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>One of the most unheralded great works of anim...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It was the Sixties, and anyone with long hair ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  My family and I normally do not watch local mo...          1\n",
       "1  Believe it or not, this was at one time the wo...          0\n",
       "2  After some internet surfing, I found the \"Home...          0\n",
       "3  One of the most unheralded great works of anim...          1\n",
       "4  It was the Sixties, and anyone with long hair ...          0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('IMDb_Reviews.csv', encoding='latin1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total sample: 50000\n"
     ]
    }
   ],
   "source": [
    "# 전체 샘플의 개수\n",
    "print('total sample:', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25,000개씩 분리\n",
    "train_df = df[:25000]\n",
    "test_df = df[25000:]\n",
    "train_df.to_csv('train_data.csv', index=False)\n",
    "test_df.to_csv('test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필드 정의하기(torchtext.data)\n",
    "\n",
    "from torchtext.legacy.data import TabularDataset\n",
    "from torchtext.legacy import data\n",
    "\n",
    "# sequential : 시퀀스 데이터 여부 (True가 기본값)\n",
    "# use_vocab : 단어 집합을 만들 것인지 여부 (True가 기본값)\n",
    "# tokenize : 어떤 토큰화 함수를 사용할 것인지 지정 (string.split이 기본값)\n",
    "# lower : 영어 데이터를 전부 소문자화함 (False가 기본값)\n",
    "# batch_first : 미니 배치 차원을 맨 앞으로 하여 데이터를 불러올 것인지 여부 (False가 기본값)\n",
    "# is_target : 레이블 데이터 여부 (False가 기본값)\n",
    "# fix_length : 최대 허용 길이. 이 길이에 맞춰서 패딩 작업(Padding)이 진행됨\n",
    "\n",
    "# 텍스트 객체\n",
    "TEXT = data.Field(\n",
    "    sequential=True, use_vocab=True, tokenize=str.split, \n",
    "    lower=True, batch_first=True, fix_length=20)\n",
    "\n",
    "# 레이블 객체\n",
    "LABEL = data.Field(\n",
    "    sequential=False, use_vocab=False, batch_first=False, is_target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train :  25000\n",
      "test :  25000\n",
      "{'text': ['my', 'family', 'and', 'i', 'normally', 'do', 'not', 'watch', 'local', 'movies', 'for', 'the', 'simple', 'reason', 'that', 'they', 'are', 'poorly', 'made,', 'they', 'lack', 'the', 'depth,', 'and', 'just', 'not', 'worth', 'our', 'time.<br', '/><br', '/>the', 'trailer', 'of', '\"nasaan', 'ka', 'man\"', 'caught', 'my', 'attention,', 'my', 'daughter', 'in', \"law's\", 'and', \"daughter's\", 'so', 'we', 'took', 'time', 'out', 'to', 'watch', 'it', 'this', 'afternoon.', 'the', 'movie', 'exceeded', 'our', 'expectations.', 'the', 'cinematography', 'was', 'very', 'good,', 'the', 'story', 'beautiful', 'and', 'the', 'acting', 'awesome.', 'jericho', 'rosales', 'was', 'really', 'very', 'good,', \"so's\", 'claudine', 'barretto.', 'the', 'fact', 'that', 'i', 'despised', 'diether', 'ocampo', 'proves', 'he', 'was', 'effective', 'at', 'his', 'role.', 'i', 'have', 'never', 'been', 'this', 'touched,', 'moved', 'and', 'affected', 'by', 'a', 'local', 'movie', 'before.', 'imagine', 'a', 'cynic', 'like', 'me', 'dabbing', 'my', 'eyes', 'at', 'the', 'end', 'of', 'the', 'movie?', 'congratulations', 'to', 'star', 'cinema!!', 'way', 'to', 'go,', 'jericho', 'and', 'claudine!!'], 'label': '1'}\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 만들기\n",
    "\n",
    "from torchtext.legacy.data import TabularDataset\n",
    "\n",
    "# path : 파일이 위치한 경로.\n",
    "# format : 데이터의 포맷.\n",
    "# fields : 위에서 정의한 필드를 지정. \n",
    "# 첫번째 원소는 데이터 셋 내에서 해당 필드를 호칭할 이름, 두번째 원소는 지정할 필드.\n",
    "# skip_header : 데이터의 첫번째 줄은 무시\n",
    "train_data, test_data = TabularDataset.splits(\n",
    "    path='.', train='train_data.csv', test='test_data.csv', format='csv',\n",
    "    fields=[('text', TEXT), ('label', LABEL)], skip_header=True)\n",
    "print('train : ', len(train_data))\n",
    "print('test : ', len(test_data))\n",
    "print(vars(train_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['my', 'family', 'and', 'i', 'normally', 'do', 'not', 'watch', 'local', 'movies', 'for', 'the', 'simple', 'reason', 'that', 'they', 'are', 'poorly', 'made,', 'they', 'lack', 'the', 'depth,', 'and', 'just', 'not', 'worth', 'our', 'time.<br', '/><br', '/>the', 'trailer', 'of', '\"nasaan', 'ka', 'man\"', 'caught', 'my', 'attention,', 'my', 'daughter', 'in', \"law's\", 'and', \"daughter's\", 'so', 'we', 'took', 'time', 'out', 'to', 'watch', 'it', 'this', 'afternoon.', 'the', 'movie', 'exceeded', 'our', 'expectations.', 'the', 'cinematography', 'was', 'very', 'good,', 'the', 'story', 'beautiful', 'and', 'the', 'acting', 'awesome.', 'jericho', 'rosales', 'was', 'really', 'very', 'good,', \"so's\", 'claudine', 'barretto.', 'the', 'fact', 'that', 'i', 'despised', 'diether', 'ocampo', 'proves', 'he', 'was', 'effective', 'at', 'his', 'role.', 'i', 'have', 'never', 'been', 'this', 'touched,', 'moved', 'and', 'affected', 'by', 'a', 'local', 'movie', 'before.', 'imagine', 'a', 'cynic', 'like', 'me', 'dabbing', 'my', 'eyes', 'at', 'the', 'end', 'of', 'the', 'movie?', 'congratulations', 'to', 'star', 'cinema!!', 'way', 'to', 'go,', 'jericho', 'and', 'claudine!!'], 'label': '1'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('text', <torchtext.legacy.data.field.Field object at 0x7fedc245beb0>), ('label', <torchtext.legacy.data.field.Field object at 0x7fedc245bb20>)])\n"
     ]
    }
   ],
   "source": [
    "# 필드 구성 확인.\n",
    "print(train_data.fields.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기: 10002\n"
     ]
    }
   ],
   "source": [
    "# 단어 집합(Vocabulary) 만들기\n",
    "\n",
    "# min_freq : 단어 집합에 추가 시 단어의 최소 등장 빈도 조건을 추가.\n",
    "# max_size : 단어 집합의 최대 크기를 지정.\n",
    "TEXT.build_vocab(train_data, min_freq=10, max_size=10000)\n",
    "\n",
    "print('단어 집합의 크기:', len(TEXT.vocab))\n",
    "\n",
    "# 단어 집합의 크기를 10,000개로 제한하였지만 실제 생성된 단어 집합의 크기는 0번 단어부터 \n",
    "# 10,001번 단어까지 총 10,002개. \n",
    "# 이는 토치텍스트가 임의로 특별 토큰인 <unk>와 <pad>를 추가하였기 때문.\n",
    "# 토치텍스트는 <unk>의 번호는 0번, <pad>의 번호는 1번을 부여함\n",
    "# <unk>는 단어 집합에 없는 단어를 표현할 때 사용되며, \n",
    "# <pad>는 길이를 맞추는 패딩 작업을 할 때 사용됨.\n",
    "# print(TEXT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data mini batch size :  5000\n",
      "test data mini batch size :  5000\n",
      "type(batch) :  <class 'torchtext.legacy.data.batch.Batch'>\n",
      "batch.text :  tensor([[   0,    7,    3,   49,  838,   20,    6,    0,   43,    0,    6,   98,\n",
      "            5,   11,   15,  386,    0,    5,    2,  501],\n",
      "        [3013,    0,   10,  150, 2144, 1444,  354,   32,  949,    6,    2,  501,\n",
      "            5,  104, 8700, 6905,    4,    0,   35,  224],\n",
      "        [   9,  205,  141,   12,  273,   55, 2236,  147,  171, 2714,    5,    3,\n",
      "          167,    5,    2,  125,  186,    0,    2, 1849],\n",
      "        [  10,   20,    7, 1190, 2440,   81,  219,    0, 1303,   17, 2611,   30,\n",
      "            4,  276,  195,   19,    3,  422, 1834,  143],\n",
      "        [  10,   25, 6137,    5,  366,  909,    4, 2654,  390, 2167, 4653,    0,\n",
      "            7,    3, 3739,  852, 1391,   16, 2035,    0]])\n"
     ]
    }
   ],
   "source": [
    "# 데이터로더 만들기\n",
    "from torchtext.legacy.data import Iterator\n",
    "\n",
    "batch_size = 5\n",
    "train_loader = Iterator(dataset=train_data, batch_size=batch_size)\n",
    "test_loader = Iterator(dataset=test_data, batch_size=batch_size)\n",
    "\n",
    "print('train data mini batch size : ', len(train_loader))\n",
    "print('test data mini batch size : ', len(test_loader))\n",
    "\n",
    "batch = next(iter(train_loader)) # 첫번째 미니배치\n",
    "print('type(batch) : ', type(batch))\n",
    "print('batch.text : ', batch.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "배치 크기가 5이기 때문에 5개의 샘플이 출력됨. 초기에 Field를 정의할 때 fix_length를 20으로 지정 했기때문에 하나의 미니 배치 크기는 20 (배치 크기 * fix_length).  \n",
    "\n",
    "각 샘플의 중간 중간에 숫자 0은 unk 토큰 번호로 단어 집합에 포함되지 못한 단어임."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pad토큰이 사용되는 경우\n",
    "pad 토큰이 어떤 경우에 사용되는지 보겠습니다. 맨 처음 필드를 정의할 때 fix_length를 20이 아니라 150으로 정의하고, 이후에는 데이터로더를 정의하기까지 모든 실습을 동일하게 진행했다고 가정해봅시다. 그리고 첫번째 미니배치의 첫번째 샘플을 출력해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   0,    7,    3,   49,  838,   20,    6,    0,   43,    0,    6,   98,\n",
      "           5,   11,   15,  386,    0,    5,    2,  501,    5,    2,  104, 1324,\n",
      "         819,    2,  236,   12,  123,  121,   23, 1075,    5,    3, 1166, 1486,\n",
      "         282,    8,   32,    0,  752,    5,    2,  759,  985,    4,   12,   30,\n",
      "           5,  116,    7,    3, 1352,  114,   22,    0,  116,   34,  248,  475,\n",
      "          37,   92,   80, 1066,    4,   10,    7,   30,    5,    2, 1806,  106,\n",
      "          10,   20,    7,  283,    6,  237,  395,  186,  124,  511, 2371,   11,\n",
      "         142,   73, 4013,   63,    8,    2,  236,   12,   30,    5,    2,  256,\n",
      "         121,    7,    3, 9436,   43,   39,  140,   12,   41,    6,   28, 3066,\n",
      "          17,   37,   92,   80, 3481,    2,  111,    2,  121, 7647,    7, 1265,\n",
      "           0,   52,   23, 2830, 7081,    4, 4633,   12,   23,  950,    6,  166,\n",
      "         761,   12,   39,   83, 1113,   65,  837,    2,   25,   17,    3, 1420,\n",
      "         325,    5,   22,  599,  504,  105])\n"
     ]
    }
   ],
   "source": [
    "TEXT = data.Field(\n",
    "    sequential=True, use_vocab=True, tokenize=str.split, \n",
    "    lower=True, batch_first=True, fix_length=150)\n",
    "\n",
    "# 레이블 객체\n",
    "LABEL = data.Field(\n",
    "    sequential=False, use_vocab=False, batch_first=False, is_target=True)\n",
    "\n",
    "train_data, test_data = TabularDataset.splits(\n",
    "    path='.', train='train_data.csv', test='test_data.csv', format='csv',\n",
    "    fields=[('text', TEXT), ('label', LABEL)], skip_header=True)\n",
    "TEXT.build_vocab(train_data, min_freq=10, max_size=10000)\n",
    "batch_size = 5\n",
    "train_loader = Iterator(dataset=train_data, batch_size=batch_size)\n",
    "\n",
    "batch = next(iter(train_loader)) # 첫번째 미니배치\n",
    "print(batch.text[0]) # 첫번째 미니배치 중 첫번째 샘플"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기존 샘플의 길이가 150이 되지 않았기 때문에 뒷 부분이 <pad> 번호인 1로 채워짐. 이처럼 서로 다른 길이의 샘플을 동일한 길이로 맞춰주는 작업을 패딩 작업이라고 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토치텍스트 튜토리얼(Torchtext tutorial) - 한국어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이버 영화 리뷰 데이터 다운로드\n",
    "\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")\n",
    "\n",
    "train_df = pd.read_table('ratings_train.txt')\n",
    "test_df = pd.read_table('ratings_test.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train sample :  150000\n",
      "test sample :  50000\n"
     ]
    }
   ],
   "source": [
    "print('train sample : ', len(train_df))\n",
    "print('test sample : ', len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필드 정의하기(torchtext.data)\n",
    "from torchtext.legacy import data\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "# Mecab을 토크나이저로 사용\n",
    "tokenizer = Mecab()\n",
    "\n",
    "# 필드 정의\n",
    "ID = data.Field(sequential=False, use_vocab=False)\n",
    "TEXT = data.Field(sequential=True, use_vocab=True, tokenize=tokenizer.morphs,\n",
    "                  lower=True, batch_first=True, fix_length=20)\n",
    "LABEL = data.Field(sequential=False, use_vocab=False, is_target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train sample :  150000\n",
      "test sample :  50000\n",
      "vars(train_data[0]) :  {'id': '9976970', 'text': ['아', '더', '빙', '.', '.', '진짜', '짜증', '나', '네요', '목소리'], 'label': '0'}\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 만들기\n",
    "from torchtext.legacy.data import TabularDataset\n",
    "\n",
    "# 데이터 로드 및 토큰화\n",
    "train_data, test_data = TabularDataset.splits(\n",
    "    path='.', train='ratings_train.txt', test='ratings_test.txt', format='tsv',\n",
    "    fields=[('id', ID), ('text', TEXT), ('label', LABEL)], skip_header=True)\n",
    "\n",
    "print('train sample : ', len(train_data))\n",
    "print('test sample : ', len(test_data))\n",
    "print('vars(train_data[0]) : ', vars(train_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 :  10002\n"
     ]
    }
   ],
   "source": [
    "# 단어 집합(Vocabulary) 만들기\n",
    "\n",
    "# min_freq : 단어 집합에 추가 시 단어의 최소 등장 빈도 조건을 추가.\n",
    "# max_size : 단어 집합의 최대 크기를 지정.\n",
    "TEXT.build_vocab(train_data, min_freq=10, max_size=10000)\n",
    "print('단어 집합의 크기 : ', len(TEXT.vocab))\n",
    "# print(TEXT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 미니 배치 수 : 30000\n",
      "테스트 데이터의 미니 배치 수 : 10000\n",
      "batch.text :  tensor([[ 232,   11, 6338,    4,    5,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   3,    5,   23,   15,  126,    3,    5,  235,   10,  433,   12, 3481,\n",
      "           30,   25,  325,  682,   86, 2454, 8140,   40],\n",
      "        [   5,   15,   31,  240,  273,   37, 1294,    3,  388,   11,   53,   55,\n",
      "            6,    2,    1,    1,    1,    1,    1,    1],\n",
      "        [4094, 2004,    9,   35,    8,    7,    0,   14, 2038,    4, 3043,   34,\n",
      "           63, 2164,    1,    1,    1,    1,    1,    1],\n",
      "        [   0,   39,    0,    5,   23,    0,  146,    8,    7,   15,   51,  404,\n",
      "           49,  158,  139,  409,   92,  725, 2700,   14]])\n"
     ]
    }
   ],
   "source": [
    "from torchtext.legacy.data import Iterator\n",
    "batch_size = 5\n",
    "train_loader = Iterator(dataset=train_data, batch_size = batch_size)\n",
    "test_loader = Iterator(dataset=test_data, batch_size = batch_size)\n",
    "\n",
    "print('훈련 데이터의 미니 배치 수 : {}'.format(len(train_loader)))\n",
    "print('테스트 데이터의 미니 배치 수 : {}'.format(len(test_loader)))\n",
    "\n",
    "batch = next(iter(train_loader)) # 첫번째 미니배치\n",
    "print('batch.text : ', batch.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "배치 크기가 5이기 때문에 5개의 샘플이 출력됨. 각 샘플의 길이는 20의 길이를 가지는데, 이는 앞서 초기에 필드를 정의할 때 fix_length를 20으로 정해주었기 때문. 하나의 미니 배치의 크기는 (배치 크기 × fix_length)임.  \n",
    "\n",
    "샘플의 중간, 중간에는 숫자 0이 존재하는데 이는 단어 집합에 포함되지 못한 단어들은 unk라는 토큰으로 변환되었음을 의미함. 또한 기존 샘플 길이가 20보다 작았던 샘플들은 뒤에 pad토큰의 번호인 숫자 1로 패딩됨"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토치텍스트(TorchText)의 batch_first  \n",
    "토치텍스트에서 배치퍼스트(batch_first)를 True로 한 경우와 False를 한 경우를 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch.text :  tensor([[   0,    7,    3,   49,  838,   20,    6,    0,   43,    0,    6,   98,\n",
      "            5,   11,   15,  386,    0,    5,    2,  501],\n",
      "        [3013,    0,   10,  150, 2144, 1444,  354,   32,  949,    6,    2,  501,\n",
      "            5,  104, 8700, 6905,    4,    0,   35,  224],\n",
      "        [   9,  205,  141,   12,  273,   55, 2236,  147,  171, 2714,    5,    3,\n",
      "          167,    5,    2,  125,  186,    0,    2, 1849],\n",
      "        [  10,   20,    7, 1190, 2440,   81,  219,    0, 1303,   17, 2611,   30,\n",
      "            4,  276,  195,   19,    3,  422, 1834,  143],\n",
      "        [  10,   25, 6137,    5,  366,  909,    4, 2654,  390, 2167, 4653,    0,\n",
      "            7,    3, 3739,  852, 1391,   16, 2035,    0]])\n",
      "batch.text.shape :  torch.Size([5, 20])\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import pandas as pd\n",
    "\n",
    "from torchtext.legacy import data\n",
    "from torchtext.legacy.data import TabularDataset\n",
    "from torchtext.legacy.data import Iterator\n",
    "\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv\", \n",
    "    filename=\"IMDb_Reviews.csv\")\n",
    "\n",
    "df = pd.read_csv('IMDb_Reviews.csv', encoding='latin1')\n",
    "\n",
    "train_df = df[:25000]\n",
    "test_df = df[25000:]\n",
    "\n",
    "train_df.to_csv(\"train_data.csv\", index=False)\n",
    "test_df.to_csv(\"test_data.csv\", index=False)\n",
    "\n",
    "# 필드 정의\n",
    "# batch_first : 미니 배치 차원을 맨 앞으로 하여 데이터를 불러올 것인지 여부\n",
    "TEXT = data.Field(\n",
    "    sequential=True, use_vocab=True, tokenize=str.split,\n",
    "    lower=True, batch_first=True, fix_length=20)\n",
    "LABEL = data.Field(\n",
    "    sequential=False, use_vocab=False, batch_first=False, is_target=True)\n",
    "\n",
    "train_data, test_data = TabularDataset.splits(\n",
    "        path='.', train='train_data.csv', test='test_data.csv', format='csv',\n",
    "        fields=[('text', TEXT), ('label', LABEL)], skip_header=True)\n",
    "\n",
    "# 정의한 필드에 build_vocab() 를 통해 10000개의 단어집합 생성\n",
    "TEXT.build_vocab(train_data, min_freq=10, max_size=10000) \n",
    "\n",
    "# 첫번째 배치 출력\n",
    "batch_size = 5\n",
    "train_loader = Iterator(dataset=train_data, batch_size = batch_size)\n",
    "batch = next(iter(train_loader)) # 첫번째 미니배치\n",
    "print('batch.text : ', batch.text)\n",
    "print('batch.text.shape : ', batch.text.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch_first = False로 하였을 경우의 텐서 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch.text :  tensor([[   0, 3013,    9,   10,   10],\n",
      "        [   7,    0,  205,   20,   25],\n",
      "        [   3,   10,  141,    7, 6137],\n",
      "        [  49,  150,   12, 1190,    5],\n",
      "        [ 838, 2144,  273, 2440,  366],\n",
      "        [  20, 1444,   55,   81,  909],\n",
      "        [   6,  354, 2236,  219,    4],\n",
      "        [   0,   32,  147,    0, 2654],\n",
      "        [  43,  949,  171, 1303,  390],\n",
      "        [   0,    6, 2714,   17, 2167],\n",
      "        [   6,    2,    5, 2611, 4653],\n",
      "        [  98,  501,    3,   30,    0],\n",
      "        [   5,    5,  167,    4,    7],\n",
      "        [  11,  104,    5,  276,    3],\n",
      "        [  15, 8700,    2,  195, 3739],\n",
      "        [ 386, 6905,  125,   19,  852],\n",
      "        [   0,    4,  186,    3, 1391],\n",
      "        [   5,    0,    0,  422,   16],\n",
      "        [   2,   35,    2, 1834, 2035],\n",
      "        [ 501,  224, 1849,  143,    0]])\n",
      "batch.text.shape :  torch.Size([20, 5])\n"
     ]
    }
   ],
   "source": [
    "# 필드 정의\n",
    "TEXT = data.Field(sequential=True, use_vocab=True, tokenize=str.split,\n",
    "                  lower=True, fix_length=20)\n",
    "LABEL = data.Field(\n",
    "    sequential=False, use_vocab=False, batch_first=False, is_target=True)\n",
    "train_data, test_data = TabularDataset.splits(\n",
    "        path='.', train='train_data.csv', test='test_data.csv', format='csv',\n",
    "        fields=[('text', TEXT), ('label', LABEL)], skip_header=True)\n",
    "\n",
    "# 정의한 필드에 build_vocab() 를 통해 10000개의 단어집합 생성\n",
    "TEXT.build_vocab(train_data, min_freq=10, max_size=10000) \n",
    "\n",
    "# 첫번째 배치 출력\n",
    "batch_size = 5\n",
    "train_loader = Iterator(dataset=train_data, batch_size = batch_size)\n",
    "\n",
    "batch = next(iter(train_loader)) # 첫번째 미니배치\n",
    "\n",
    "# 첫번째 미니 배치 출력\n",
    "print('batch.text : ', batch.text)\n",
    "# 하나의 미니 배치 크기는 (fix_length * 배치 크기)\n",
    "print('batch.text.shape : ', batch.text.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  5 2022, 06:56:58) \n[GCC 7.5.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
