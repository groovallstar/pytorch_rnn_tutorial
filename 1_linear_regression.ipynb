{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 선형 회귀란 학습 데이터와 가장 잘 맞는 하나의 직선을 찾는 일. 이때 선형 회귀의 가설(직선의 방정식)은 아래와 같은 형식을 가짐  \n",
    "$ y = Wx + b $\n",
    "- 가설의 H를 따서 y대신 다음과 같이 식을 표현하기도 함  \n",
    "$ H(x) = Wx + b $  \n",
    "- 이때 x와 곱해지는 W를 가중치(Weight)라고 하며, b를 편향(bias)이라고 함\n",
    "    - W와 b는 직선의 방정식에서 기울기와 y 절편에 해당됨"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 비용 함수(Cost function)  \n",
    "비용 함수(cost function) = 손실 함수(loss function) = 오차 함수(error function) = 목적 함수(objective function)  \n",
    "\n",
    "- 예측값들과 실제값들과의 총 오차(total error)는 어떻게 구할까?  \n",
    "수식적으로 단순히 '오차 = 실제값 - 예측값'으로 정의하면 오차값이 음수가 나오는 경우가 생김  \n",
    "그래서 오차를 그냥 전부 더하는 것이 아니라, 각 오차들을 제곱해준 뒤에 전부 더함  \n",
    "\n",
    "- 평균 제곱 오차 (Mean Squared Error, MSE)  \n",
    "오차의 제곱합에 대해 데이터의 개수인 n으로 나눈 평균값  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 옵티마이저 - 경사 하강법(Gradient Descent)  \n",
    "비용 함수(Cost Function)의 값을 최소로 하는 W와 b를 찾을 때 사용되는 것이 옵티마이저(Optimizer), 최적화 알고리즘  \n",
    "이 옵티마이저 알고리즘을 통해 적절한 W와 b를 찾아내는 과정을 머신 러닝에서 학습(training)이라고 부름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7e215b02d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "torch.Size([3, 1])\n",
      "tensor([[2.],\n",
      "        [4.],\n",
      "        [6.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "# 변수 선언\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "print(x_train)\n",
    "print(x_train.shape)\n",
    "print(y_train)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n",
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 가중치와 편향의 초기화\n",
    "# 선형 회귀의 목표는 가장 잘 맞는 직선을 정의하는 W 와 b의 값을 찾는 것\n",
    "\n",
    "# 가중치 W를 0으로 초기화하고 학습을 통해 값이 변경되는 변수임을 명시함.\n",
    "W = torch.zeros(1, requires_grad=True)\n",
    "print(W)\n",
    "\n",
    "# 편향 b도 0으로 초기화하고 학습을 통해 값이 변경되는 변수임을 명시함.\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "print(b)\n",
    "\n",
    "# 현재 가중치 W와 편향 b 둘다 0 이므로 방정식은 y = 0xW + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<AddBackward0>)\n",
      "tensor(18.6667, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 가설 세우기\n",
    "hypothesis = x_train * W + b\n",
    "print(hypothesis)\n",
    "\n",
    "# 비용함수 선언 (MSE)\n",
    "cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경사 하강법 구현. SGD는 경사 하강법의 일종.\n",
    "optimizer = optim.SGD([W, b], lr=0.01)\n",
    "\n",
    "# optimizer.zero_grad()를 실행하므로서 미분을 통해 얻은 기울기를 0으로 초기화 함.\n",
    "# 기울기를 초기화해야만 새로운 가중치 편향에 대해서 새로운 기울기를 구할 수 있음.\n",
    "# cost.backward() 함수를 호출하면 가중치 W와 편향 b에 대한 기울기가 계산됨.\n",
    "# 그 다음 경사 하강법 최적화 함수 opimizer의 step() 함수를 호출하여 인수로 들어갔던 \n",
    "# W와 b에서 리턴되는 변수들의 기울기에 학습률(learining rate) 0.01을 곱하여 빼줌으로서 업데이트함\n",
    "\n",
    "# gradient를 0으로 초기화\n",
    "optimizer.zero_grad()\n",
    "# 비용 함수를 미분하여 gradient 계산\n",
    "cost.backward()\n",
    "# W와 b를 업데이트\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1999 W: 0.187, b: 0.080 Cost: 18.666666\n",
      "Epoch  100/1999 W: 1.746, b: 0.578 Cost: 0.048171\n",
      "Epoch  200/1999 W: 1.800, b: 0.454 Cost: 0.029767\n",
      "Epoch  300/1999 W: 1.843, b: 0.357 Cost: 0.018394\n",
      "Epoch  400/1999 W: 1.876, b: 0.281 Cost: 0.011366\n",
      "Epoch  500/1999 W: 1.903, b: 0.221 Cost: 0.007024\n",
      "Epoch  600/1999 W: 1.924, b: 0.174 Cost: 0.004340\n",
      "Epoch  700/1999 W: 1.940, b: 0.136 Cost: 0.002682\n",
      "Epoch  800/1999 W: 1.953, b: 0.107 Cost: 0.001657\n",
      "Epoch  900/1999 W: 1.963, b: 0.084 Cost: 0.001024\n",
      "Epoch 1000/1999 W: 1.971, b: 0.066 Cost: 0.000633\n",
      "Epoch 1100/1999 W: 1.977, b: 0.052 Cost: 0.000391\n",
      "Epoch 1200/1999 W: 1.982, b: 0.041 Cost: 0.000242\n",
      "Epoch 1300/1999 W: 1.986, b: 0.032 Cost: 0.000149\n",
      "Epoch 1400/1999 W: 1.989, b: 0.025 Cost: 0.000092\n",
      "Epoch 1500/1999 W: 1.991, b: 0.020 Cost: 0.000057\n",
      "Epoch 1600/1999 W: 1.993, b: 0.016 Cost: 0.000035\n",
      "Epoch 1700/1999 W: 1.995, b: 0.012 Cost: 0.000022\n",
      "Epoch 1800/1999 W: 1.996, b: 0.010 Cost: 0.000013\n",
      "Epoch 1900/1999 W: 1.997, b: 0.008 Cost: 0.000008\n"
     ]
    }
   ],
   "source": [
    "# 전체 코드\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "# 모델 초기화\n",
    "W = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=0.01)\n",
    "\n",
    "nb_epochs = 1999 # 원하는만큼 경사 하강법을 반복\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    hypothesis = x_train * W + b\n",
    "\n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W.item(), b.item(), cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w 미분 값 : tensor(2.)\n",
      "w 미분 값 : tensor(4.)\n",
      "w 미분 값 : tensor(6.)\n",
      "w 미분 값 : tensor(8.)\n",
      "w 미분 값 : tensor(10.)\n",
      "w 미분 값 : tensor(12.)\n",
      "w 미분 값 : tensor(14.)\n",
      "w 미분 값 : tensor(16.)\n",
      "w 미분 값 : tensor(18.)\n",
      "w 미분 값 : tensor(20.)\n",
      "w 미분 값 : tensor(22.)\n",
      "w 미분 값 : tensor(24.)\n",
      "w 미분 값 : tensor(26.)\n",
      "w 미분 값 : tensor(28.)\n",
      "w 미분 값 : tensor(30.)\n",
      "w 미분 값 : tensor(32.)\n",
      "w 미분 값 : tensor(34.)\n",
      "w 미분 값 : tensor(36.)\n",
      "w 미분 값 : tensor(38.)\n",
      "w 미분 값 : tensor(40.)\n",
      "w 미분 값 : tensor(42.)\n"
     ]
    }
   ],
   "source": [
    "# optimizer.zero_grad()가 필요한 이유\n",
    "w = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs+1):\n",
    "    z = 2 * w\n",
    "    z.backward()\n",
    "    # 계속해서 미분값인 2가 누적되기 때문에 0으로 초기화시켜줘야 함\n",
    "    print('w 미분 값 :', w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수식을 w로 미분한 값: tensor(8.)\n"
     ]
    }
   ],
   "source": [
    "# 자동 미분\n",
    "# requires_grad=True는 이 텐서에 대한 기울기 저장.\n",
    "w = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "y = w**2\n",
    "z = 2*y + 5\n",
    "\n",
    "z.backward() # w에 대한 기울기를 계산\n",
    "print('수식을 w로 미분한 값:', w.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다중 선형 회귀  \n",
    "$ H(x) = w_1x_1 + w_2x_2 + w_3x_3 + b $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 w1: 0.294 w2: 0.294 w3: 0.297 b: 0.003 Cost: 29661.800781\n",
      "Epoch  100/1000 w1: 0.674 w2: 0.661 w3: 0.676 b: 0.008 Cost: 1.563628\n",
      "Epoch  200/1000 w1: 0.679 w2: 0.655 w3: 0.677 b: 0.008 Cost: 1.497595\n",
      "Epoch  300/1000 w1: 0.684 w2: 0.649 w3: 0.677 b: 0.008 Cost: 1.435044\n",
      "Epoch  400/1000 w1: 0.689 w2: 0.643 w3: 0.678 b: 0.008 Cost: 1.375726\n",
      "Epoch  500/1000 w1: 0.694 w2: 0.638 w3: 0.678 b: 0.009 Cost: 1.319507\n",
      "Epoch  600/1000 w1: 0.699 w2: 0.633 w3: 0.679 b: 0.009 Cost: 1.266222\n",
      "Epoch  700/1000 w1: 0.704 w2: 0.627 w3: 0.679 b: 0.009 Cost: 1.215703\n",
      "Epoch  800/1000 w1: 0.709 w2: 0.622 w3: 0.679 b: 0.009 Cost: 1.167810\n",
      "Epoch  900/1000 w1: 0.713 w2: 0.617 w3: 0.680 b: 0.009 Cost: 1.122429\n",
      "Epoch 1000/1000 w1: 0.718 w2: 0.613 w3: 0.680 b: 0.009 Cost: 1.079390\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터\n",
    "x1_train = torch.FloatTensor([[73], [93], [89], [96], [73]])\n",
    "x2_train = torch.FloatTensor([[80], [88], [91], [98], [66]])\n",
    "x3_train = torch.FloatTensor([[75], [93], [90], [100], [70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "# 가중치 w와 편향 b 초기화\n",
    "w1 = torch.zeros(1, requires_grad=True)\n",
    "w2 = torch.zeros(1, requires_grad=True)\n",
    "w3 = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([w1, w2, w3, b], lr=1e-5)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs+1):\n",
    "    # H(x) \n",
    "    hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b\n",
    "    # cost \n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    # cost로 H(x) 계산\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} w1: {:.3f} w2: {:.3f} w3: {:.3f} b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, w1.item(), w2.item(), w3.item(), b.item(), cost.item()))\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 백터 내적  \n",
    "행렬의 곱셈 과정에서 이루어지는 벡터 연산  \n",
    "$ H(X) = w_1x_1 + w_2x_2 + w_3x_3 $ 는 아래와 같이 벡터의 내적으로 표현함  \n",
    "$ \\begin{pmatrix} x_1 & x_2 & x_3 \\end{pmatrix} \\cdot \\begin{pmatrix} w_1 \\\\ w_2 \\\\ w_3 \\end{pmatrix} = (x_1w_1 + x_2w_2 + x_3w_3) $  \n",
    "H(X) = XW\n",
    "\n",
    "- 5개의 데이터, feature가 3일 때 독립 변수 x 개수 = (5 x 3) = 15  \n",
    "$ \n",
    "\\begin{pmatrix} x_{11} & x_{12} & x_{13} \\\\ x_{21} & x_{22} & x_{23} \\\\ x_{31} & x_{32} & x_{33} \\\\ x_{41} & x_{42} & x_{43} \\\\ x_{51} & x_{52} & x_{53}\\end{pmatrix}\n",
    "\\cdot\n",
    "\\begin{pmatrix} w_1 \\\\ w_2 \\\\ w_3 \\end{pmatrix} + \\begin{pmatrix} b \\\\ b \\\\ b \\\\ b \\\\ b \\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix} x_{11}w_1 + x_{12}w_2 + x_{13}w_3 + b \\\\ x_{21}w_1 + x_{22}w_2 + x_{23}w_3 + b \\\\ x_{31}w_1 + x_{32}w_2 + x_{33}w_3 + b \\\\ x_{41}w_1 + x_{42}w_2 + x_{43}w_3 + b \\\\ x_{51}w_1 + x_{52}w_2 + x_{53}w_3 + b\\end{pmatrix} $  \n",
    "H(X) = XW + B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "torch.Size([5, 1])\n",
      "Epoch    0/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
      "Epoch    1/20 hypothesis: tensor([66.7178, 80.1701, 76.1025, 86.0194, 61.1565]) Cost: 9537.694336\n",
      "Epoch    2/20 hypothesis: tensor([104.5421, 125.6208, 119.2478, 134.7861,  95.8280]) Cost: 3069.590820\n",
      "Epoch    3/20 hypothesis: tensor([125.9858, 151.3882, 143.7087, 162.4333, 115.4844]) Cost: 990.670288\n",
      "Epoch    4/20 hypothesis: tensor([138.1429, 165.9963, 157.5768, 178.1071, 126.6283]) Cost: 322.481964\n",
      "Epoch    5/20 hypothesis: tensor([145.0350, 174.2780, 165.4395, 186.9928, 132.9461]) Cost: 107.717064\n",
      "Epoch    6/20 hypothesis: tensor([148.9423, 178.9731, 169.8976, 192.0301, 136.5279]) Cost: 38.687401\n",
      "Epoch    7/20 hypothesis: tensor([151.1574, 181.6347, 172.4254, 194.8856, 138.5585]) Cost: 16.499046\n",
      "Epoch    8/20 hypothesis: tensor([152.4131, 183.1435, 173.8590, 196.5042, 139.7097]) Cost: 9.365656\n",
      "Epoch    9/20 hypothesis: tensor([153.1250, 183.9988, 174.6723, 197.4216, 140.3625]) Cost: 7.071105\n",
      "Epoch   10/20 hypothesis: tensor([153.5285, 184.4835, 175.1338, 197.9415, 140.7325]) Cost: 6.331867\n",
      "Epoch   11/20 hypothesis: tensor([153.7572, 184.7582, 175.3958, 198.2360, 140.9424]) Cost: 6.092532\n",
      "Epoch   12/20 hypothesis: tensor([153.8868, 184.9138, 175.5449, 198.4026, 141.0614]) Cost: 6.013823\n",
      "Epoch   13/20 hypothesis: tensor([153.9602, 185.0019, 175.6299, 198.4969, 141.1288]) Cost: 5.986775\n",
      "Epoch   14/20 hypothesis: tensor([154.0017, 185.0517, 175.6785, 198.5501, 141.1671]) Cost: 5.976314\n",
      "Epoch   15/20 hypothesis: tensor([154.0252, 185.0798, 175.7065, 198.5800, 141.1888]) Cost: 5.971213\n",
      "Epoch   16/20 hypothesis: tensor([154.0385, 185.0956, 175.7229, 198.5966, 141.2012]) Cost: 5.967797\n",
      "Epoch   17/20 hypothesis: tensor([154.0459, 185.1045, 175.7326, 198.6058, 141.2082]) Cost: 5.964961\n",
      "Epoch   18/20 hypothesis: tensor([154.0501, 185.1094, 175.7386, 198.6108, 141.2122]) Cost: 5.962292\n",
      "Epoch   19/20 hypothesis: tensor([154.0524, 185.1120, 175.7424, 198.6134, 141.2145]) Cost: 5.959693\n",
      "Epoch   20/20 hypothesis: tensor([154.0536, 185.1134, 175.7451, 198.6146, 141.2158]) Cost: 5.957091\n"
     ]
    }
   ],
   "source": [
    "x_train  =  torch.FloatTensor([[73, 80, 75], \n",
    "                               [93, 88, 93], \n",
    "                               [89, 91, 80], \n",
    "                               [96, 98, 100],   \n",
    "                               [73, 66, 70]])  \n",
    "y_train  =  torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "print(x_train.shape) # (5, 3)\n",
    "print(y_train.shape) # (5, 1)\n",
    "\n",
    "# 가중치와 편향 선언 (3, 1)\n",
    "# 행렬의 곱셈이 성립되려면 곱셈의 좌측에 있는 행렬의 열의 크기(5, 3)와 \n",
    "# 우측에 있는 행렬의 행의 크기가 일치해야 함\n",
    "W = torch.zeros((3, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=1e-5)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    # H(x) 계산. b는 브로드 캐스팅되어 각 샘플에 더해짐\n",
    "    hypothesis = x_train.matmul(W) + b\n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    # cost로 H(x) 갱신\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(\n",
    "        epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Module을 이용한 선형 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7e215b02d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.5153]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.4414], requires_grad=True)]\n",
      "Epoch    0/2000 Cost: 13.103541\n",
      "Epoch  100/2000 Cost: 0.002791\n",
      "Epoch  200/2000 Cost: 0.001724\n",
      "Epoch  300/2000 Cost: 0.001066\n",
      "Epoch  400/2000 Cost: 0.000658\n",
      "Epoch  500/2000 Cost: 0.000407\n",
      "Epoch  600/2000 Cost: 0.000251\n",
      "Epoch  700/2000 Cost: 0.000155\n",
      "Epoch  800/2000 Cost: 0.000096\n",
      "Epoch  900/2000 Cost: 0.000059\n",
      "Epoch 1000/2000 Cost: 0.000037\n",
      "Epoch 1100/2000 Cost: 0.000023\n",
      "Epoch 1200/2000 Cost: 0.000014\n",
      "Epoch 1300/2000 Cost: 0.000009\n",
      "Epoch 1400/2000 Cost: 0.000005\n",
      "Epoch 1500/2000 Cost: 0.000003\n",
      "Epoch 1600/2000 Cost: 0.000002\n",
      "Epoch 1700/2000 Cost: 0.000001\n",
      "Epoch 1800/2000 Cost: 0.000001\n",
      "Epoch 1900/2000 Cost: 0.000000\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "\n",
    "# 단순 선형 회귀이므로 input_dim=1, output_dim=1\n",
    "model = nn.Linear(1, 1)\n",
    "\n",
    "# 첫번째 W, 두번째 b. 두값 모두 랜덤 초기화\n",
    "print(list(model.parameters()))\n",
    "\n",
    "# optimizer 설정. \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs):\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train) # 평균제곱오차 함수\n",
    "    \n",
    "    optimizer.zero_grad() # gradient 초기화\n",
    "    cost.backward() # 미분\n",
    "    optimizer.step() # W, b update\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch, nb_epochs, cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 후 입력이 4일 때의 예측값 : tensor([[7.9989]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 임의의 값 4를 입력\n",
    "new_var = torch.FloatTensor([[4.0]])\n",
    "# 입력한 값 4에 대해 예측값 y를 리턴받아 pred_y에 저장\n",
    "pred_y = model(new_var) # forward 연산\n",
    "# y = 2x 이므로 입력이 4라면 y가 8에 가까운 값이 나와야 제대로 학습이 된 것\n",
    "print(\"훈련 후 입력이 4일 때의 예측값 :\", pred_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[1.9994]], requires_grad=True), Parameter containing:\n",
      "tensor([0.0014], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(list((model.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-0.1119,  0.2710, -0.5435]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3462], requires_grad=True)]\n",
      "Epoch    0/2000 Cost: 42134.707031\n",
      "Epoch  100/2000 Cost: 5.960053\n",
      "Epoch  200/2000 Cost: 5.654689\n",
      "Epoch  300/2000 Cost: 5.365413\n",
      "Epoch  400/2000 Cost: 5.091413\n",
      "Epoch  500/2000 Cost: 4.831831\n",
      "Epoch  600/2000 Cost: 4.585963\n",
      "Epoch  700/2000 Cost: 4.353061\n",
      "Epoch  800/2000 Cost: 4.132451\n",
      "Epoch  900/2000 Cost: 3.923452\n",
      "Epoch 1000/2000 Cost: 3.725488\n",
      "Epoch 1100/2000 Cost: 3.537961\n",
      "Epoch 1200/2000 Cost: 3.360339\n",
      "Epoch 1300/2000 Cost: 3.192085\n",
      "Epoch 1400/2000 Cost: 3.032697\n",
      "Epoch 1500/2000 Cost: 2.881700\n",
      "Epoch 1600/2000 Cost: 2.738672\n",
      "Epoch 1700/2000 Cost: 2.603207\n",
      "Epoch 1800/2000 Cost: 2.474846\n",
      "Epoch 1900/2000 Cost: 2.353291\n",
      "Epoch 2000/2000 Cost: 2.238137\n"
     ]
    }
   ],
   "source": [
    "# 다중 회귀 구현\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "model = nn.Linear(3, 1)\n",
    "print(list(model.parameters()))\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)\n",
    "\n",
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    prediction = model(x_train)\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch, nb_epochs, cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 후 입력이 73, 80, 75일 때의 예측값 : tensor([[153.7184]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 임의의 입력 [73, 80, 75]를 선언\n",
    "new_var =  torch.FloatTensor([[73, 80, 75]]) \n",
    "# 입력한 값 [73, 80, 75]에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n",
    "pred_y = model(new_var) \n",
    "print(\"훈련 후 입력이 73, 80, 75일 때의 예측값 :\", pred_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.8541, 0.8475, 0.3096]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3568], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.parameters()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "클래스로 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7e215b02d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 Cost: 13.103541\n",
      "Epoch  100/2000 Cost: 0.002791\n",
      "Epoch  200/2000 Cost: 0.001724\n",
      "Epoch  300/2000 Cost: 0.001066\n",
      "Epoch  400/2000 Cost: 0.000658\n",
      "Epoch  500/2000 Cost: 0.000407\n",
      "Epoch  600/2000 Cost: 0.000251\n",
      "Epoch  700/2000 Cost: 0.000155\n",
      "Epoch  800/2000 Cost: 0.000096\n",
      "Epoch  900/2000 Cost: 0.000059\n",
      "Epoch 1000/2000 Cost: 0.000037\n",
      "Epoch 1100/2000 Cost: 0.000023\n",
      "Epoch 1200/2000 Cost: 0.000014\n",
      "Epoch 1300/2000 Cost: 0.000009\n",
      "Epoch 1400/2000 Cost: 0.000005\n",
      "Epoch 1500/2000 Cost: 0.000003\n",
      "Epoch 1600/2000 Cost: 0.000002\n",
      "Epoch 1700/2000 Cost: 0.000001\n",
      "Epoch 1800/2000 Cost: 0.000001\n",
      "Epoch 1900/2000 Cost: 0.000000\n",
      "Epoch 2000/2000 Cost: 0.000000\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "    def forward(self, x):\n",
    "        \"\"\"forward 연산\"\"\"\n",
    "        return self.linear(x)\n",
    "\n",
    "model = LinearRegressionModel()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs+1):\n",
    "    prediction = model(x_train)\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch, nb_epochs, cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 Cost: 42134.707031\n",
      "Epoch  100/2000 Cost: 5.960053\n",
      "Epoch  200/2000 Cost: 5.654689\n",
      "Epoch  300/2000 Cost: 5.365413\n",
      "Epoch  400/2000 Cost: 5.091413\n",
      "Epoch  500/2000 Cost: 4.831831\n",
      "Epoch  600/2000 Cost: 4.585963\n",
      "Epoch  700/2000 Cost: 4.353061\n",
      "Epoch  800/2000 Cost: 4.132451\n",
      "Epoch  900/2000 Cost: 3.923452\n",
      "Epoch 1000/2000 Cost: 3.725488\n",
      "Epoch 1100/2000 Cost: 3.537961\n",
      "Epoch 1200/2000 Cost: 3.360339\n",
      "Epoch 1300/2000 Cost: 3.192085\n",
      "Epoch 1400/2000 Cost: 3.032697\n",
      "Epoch 1500/2000 Cost: 2.881700\n",
      "Epoch 1600/2000 Cost: 2.738672\n",
      "Epoch 1700/2000 Cost: 2.603207\n",
      "Epoch 1800/2000 Cost: 2.474846\n",
      "Epoch 1900/2000 Cost: 2.353291\n",
      "Epoch 2000/2000 Cost: 2.238137\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "class MultivariateLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 다중 선형 회귀이므로 input_dim=3, output_dim=1.\n",
    "        self.linear = nn.Linear(3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "model = MultivariateLinearRegressionModel()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)\n",
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs+1):\n",
    "    prediction = model(x_train)\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch, nb_epochs, cost.item()))"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미니 배치와 데이터 로드  \n",
    "![image.png](attachment:image.png)  \n",
    "- 에포크(Epoch) : 전체 훈련 데이터가 학습에 한번 사용된 주기\n",
    "- 배치 크기(batch size)\n",
    "    - 미니 배치의 개수만큼 경사하강법을 수행해야 전체 데이터가 한 번 전부 사용됨\n",
    "    - 미니 배치의 개수는 결국 미니 배치의 크기를 몇으로 하느냐에 따라 달라짐\n",
    "    - 전체 데이터에 대해 한번에 경사 하강법을 수행하는 방법 : 배치 경사 하강법\n",
    "        - 전체 데이터를 사용하므로 가중치 값이 최적값에 수렴하는 과정이 안정적\n",
    "    - 미니 배치 단위로 경사 하강법을 수행하는 방법 : 미니 배치 경사 하강법\n",
    "        - 전체 데이터의 일부만을 보고 수행하므로 최적값으로 수렴하는 과정에서 조금 헤매기도 하지만 훈련 속도가 빠름\n",
    "- 배치 크기는 보통 2의 제곱수를 사용. 그 이유는 CPU와 GPU의 메모리가 2의 배수이므로"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이터레이션  \n",
    "![image.png](attachment:image.png)  \n",
    "한번의 에포크 내에서 이뤄지는 가중치 W와 b의 업데이트 횟수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "dataset = TensorDataset(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle=True를 선택하면 Epoch마다 데이터셋을 섞어서 데이터가 학습되는 순서를 바꿉니다.\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(3, 1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Batch 1/3 Cost: 26838.554688\n",
      "Epoch    0/20 Batch 2/3 Cost: 4949.356934\n",
      "Epoch    0/20 Batch 3/3 Cost: 1217.417969\n",
      "Epoch    1/20 Batch 1/3 Cost: 874.722717\n",
      "Epoch    1/20 Batch 2/3 Cost: 256.657043\n",
      "Epoch    1/20 Batch 3/3 Cost: 24.682867\n",
      "Epoch    2/20 Batch 1/3 Cost: 45.678944\n",
      "Epoch    2/20 Batch 2/3 Cost: 5.590558\n",
      "Epoch    2/20 Batch 3/3 Cost: 27.639778\n",
      "Epoch    3/20 Batch 1/3 Cost: 8.685300\n",
      "Epoch    3/20 Batch 2/3 Cost: 3.939066\n",
      "Epoch    3/20 Batch 3/3 Cost: 2.802620\n",
      "Epoch    4/20 Batch 1/3 Cost: 6.150103\n",
      "Epoch    4/20 Batch 2/3 Cost: 7.124768\n",
      "Epoch    4/20 Batch 3/3 Cost: 2.149211\n",
      "Epoch    5/20 Batch 1/3 Cost: 1.789171\n",
      "Epoch    5/20 Batch 2/3 Cost: 15.624266\n",
      "Epoch    5/20 Batch 3/3 Cost: 3.540388\n",
      "Epoch    6/20 Batch 1/3 Cost: 5.700332\n",
      "Epoch    6/20 Batch 2/3 Cost: 6.880760\n",
      "Epoch    6/20 Batch 3/3 Cost: 2.429880\n",
      "Epoch    7/20 Batch 1/3 Cost: 7.628876\n",
      "Epoch    7/20 Batch 2/3 Cost: 3.857748\n",
      "Epoch    7/20 Batch 3/3 Cost: 5.840155\n",
      "Epoch    8/20 Batch 1/3 Cost: 5.205458\n",
      "Epoch    8/20 Batch 2/3 Cost: 2.935290\n",
      "Epoch    8/20 Batch 3/3 Cost: 16.427006\n",
      "Epoch    9/20 Batch 1/3 Cost: 3.824505\n",
      "Epoch    9/20 Batch 2/3 Cost: 6.187510\n",
      "Epoch    9/20 Batch 3/3 Cost: 7.496263\n",
      "Epoch   10/20 Batch 1/3 Cost: 4.572945\n",
      "Epoch   10/20 Batch 2/3 Cost: 6.737031\n",
      "Epoch   10/20 Batch 3/3 Cost: 5.895672\n",
      "Epoch   11/20 Batch 1/3 Cost: 1.443226\n",
      "Epoch   11/20 Batch 2/3 Cost: 9.311169\n",
      "Epoch   11/20 Batch 3/3 Cost: 8.005629\n",
      "Epoch   12/20 Batch 1/3 Cost: 6.552180\n",
      "Epoch   12/20 Batch 2/3 Cost: 3.549880\n",
      "Epoch   12/20 Batch 3/3 Cost: 7.657715\n",
      "Epoch   13/20 Batch 1/3 Cost: 7.357927\n",
      "Epoch   13/20 Batch 2/3 Cost: 3.529320\n",
      "Epoch   13/20 Batch 3/3 Cost: 10.374599\n",
      "Epoch   14/20 Batch 1/3 Cost: 3.878338\n",
      "Epoch   14/20 Batch 2/3 Cost: 5.393184\n",
      "Epoch   14/20 Batch 3/3 Cost: 13.809702\n",
      "Epoch   15/20 Batch 1/3 Cost: 5.501532\n",
      "Epoch   15/20 Batch 2/3 Cost: 4.437713\n",
      "Epoch   15/20 Batch 3/3 Cost: 13.423565\n",
      "Epoch   16/20 Batch 1/3 Cost: 4.113708\n",
      "Epoch   16/20 Batch 2/3 Cost: 6.071901\n",
      "Epoch   16/20 Batch 3/3 Cost: 7.803999\n",
      "Epoch   17/20 Batch 1/3 Cost: 2.076509\n",
      "Epoch   17/20 Batch 2/3 Cost: 14.499167\n",
      "Epoch   17/20 Batch 3/3 Cost: 4.352218\n",
      "Epoch   18/20 Batch 1/3 Cost: 7.422428\n",
      "Epoch   18/20 Batch 2/3 Cost: 3.180331\n",
      "Epoch   18/20 Batch 3/3 Cost: 10.140516\n",
      "Epoch   19/20 Batch 1/3 Cost: 5.294771\n",
      "Epoch   19/20 Batch 2/3 Cost: 6.063771\n",
      "Epoch   19/20 Batch 3/3 Cost: 4.648559\n",
      "Epoch   20/20 Batch 1/3 Cost: 7.303863\n",
      "Epoch   20/20 Batch 2/3 Cost: 3.836170\n",
      "Epoch   20/20 Batch 3/3 Cost: 5.569603\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    for batch_idx, samples in enumerate(dataloader):\n",
    "        # print(batch_idx)\n",
    "        # print(samples)\n",
    "        x_train, y_train = samples\n",
    "        # H(x) 계산\n",
    "        prediction = model(x_train)\n",
    "\n",
    "        # cost 계산\n",
    "        cost = F.mse_loss(prediction, y_train)\n",
    "\n",
    "        # cost로 H(x) 계산\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, batch_idx+1, len(dataloader), cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 후 입력이 73, 80, 75일 때 예측: tensor([[153.5408]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "new_var = torch.FloatTensor([[73, 80, 75]])\n",
    "pred_y = model(new_var)\n",
    "print('훈련 후 입력이 73, 80, 75일 때 예측:', pred_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "커스텀 데이터셋(Custom Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 커스텀 데이터 셋 뼈대\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        \"\"\"데이터 셋의 전처리를 해주는 부분\"\"\"\n",
    "    def __len__(self):\n",
    "        \"\"\"데이터 셋의 길이. 총 샘플의 수\n",
    "           len(dataset)을 했을 때 데이터셋의 크기를 리턴\n",
    "        \"\"\"\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"데이터 셋에서 특정 1개의 샘플을 가져오는 부분\n",
    "           dataset[i]을 했을 때 i번째 샘플을 가져오도록 하는 인덱싱\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Batch 1/3 Cost: 54524.218750\n",
      "Epoch    0/20 Batch 2/3 Cost: 8945.384766\n",
      "Epoch    0/20 Batch 3/3 Cost: 2403.548828\n",
      "Epoch    1/20 Batch 1/3 Cost: 1564.037354\n",
      "Epoch    1/20 Batch 2/3 Cost: 550.309875\n",
      "Epoch    1/20 Batch 3/3 Cost: 128.874252\n",
      "Epoch    2/20 Batch 1/3 Cost: 35.747013\n",
      "Epoch    2/20 Batch 2/3 Cost: 24.368717\n",
      "Epoch    2/20 Batch 3/3 Cost: 13.028359\n",
      "Epoch    3/20 Batch 1/3 Cost: 0.577794\n",
      "Epoch    3/20 Batch 2/3 Cost: 5.028281\n",
      "Epoch    3/20 Batch 3/3 Cost: 1.949444\n",
      "Epoch    4/20 Batch 1/3 Cost: 2.790054\n",
      "Epoch    4/20 Batch 2/3 Cost: 4.539859\n",
      "Epoch    4/20 Batch 3/3 Cost: 1.471622\n",
      "Epoch    5/20 Batch 1/3 Cost: 0.735318\n",
      "Epoch    5/20 Batch 2/3 Cost: 2.351956\n",
      "Epoch    5/20 Batch 3/3 Cost: 7.989922\n",
      "Epoch    6/20 Batch 1/3 Cost: 2.292079\n",
      "Epoch    6/20 Batch 2/3 Cost: 3.429396\n",
      "Epoch    6/20 Batch 3/3 Cost: 0.660007\n",
      "Epoch    7/20 Batch 1/3 Cost: 1.045065\n",
      "Epoch    7/20 Batch 2/3 Cost: 4.478187\n",
      "Epoch    7/20 Batch 3/3 Cost: 0.089189\n",
      "Epoch    8/20 Batch 1/3 Cost: 0.152518\n",
      "Epoch    8/20 Batch 2/3 Cost: 4.536362\n",
      "Epoch    8/20 Batch 3/3 Cost: 1.777479\n",
      "Epoch    9/20 Batch 1/3 Cost: 2.310009\n",
      "Epoch    9/20 Batch 2/3 Cost: 4.838332\n",
      "Epoch    9/20 Batch 3/3 Cost: 0.447243\n",
      "Epoch   10/20 Batch 1/3 Cost: 1.091359\n",
      "Epoch   10/20 Batch 2/3 Cost: 6.368413\n",
      "Epoch   10/20 Batch 3/3 Cost: 0.283574\n",
      "Epoch   11/20 Batch 1/3 Cost: 3.047754\n",
      "Epoch   11/20 Batch 2/3 Cost: 2.587975\n",
      "Epoch   11/20 Batch 3/3 Cost: 0.074560\n",
      "Epoch   12/20 Batch 1/3 Cost: 4.470545\n",
      "Epoch   12/20 Batch 2/3 Cost: 0.923764\n",
      "Epoch   12/20 Batch 3/3 Cost: 0.132706\n",
      "Epoch   13/20 Batch 1/3 Cost: 1.450770\n",
      "Epoch   13/20 Batch 2/3 Cost: 4.055173\n",
      "Epoch   13/20 Batch 3/3 Cost: 1.431692\n",
      "Epoch   14/20 Batch 1/3 Cost: 3.468175\n",
      "Epoch   14/20 Batch 2/3 Cost: 3.627177\n",
      "Epoch   14/20 Batch 3/3 Cost: 1.006021\n",
      "Epoch   15/20 Batch 1/3 Cost: 2.649999\n",
      "Epoch   15/20 Batch 2/3 Cost: 0.482014\n",
      "Epoch   15/20 Batch 3/3 Cost: 5.925796\n",
      "Epoch   16/20 Batch 1/3 Cost: 1.667954\n",
      "Epoch   16/20 Batch 2/3 Cost: 3.310733\n",
      "Epoch   16/20 Batch 3/3 Cost: 4.122568\n",
      "Epoch   17/20 Batch 1/3 Cost: 4.719289\n",
      "Epoch   17/20 Batch 2/3 Cost: 2.107971\n",
      "Epoch   17/20 Batch 3/3 Cost: 0.143212\n",
      "Epoch   18/20 Batch 1/3 Cost: 5.055257\n",
      "Epoch   18/20 Batch 2/3 Cost: 0.843501\n",
      "Epoch   18/20 Batch 3/3 Cost: 3.317149\n",
      "Epoch   19/20 Batch 1/3 Cost: 5.118366\n",
      "Epoch   19/20 Batch 2/3 Cost: 1.420750\n",
      "Epoch   19/20 Batch 3/3 Cost: 0.311976\n",
      "Epoch   20/20 Batch 1/3 Cost: 4.522241\n",
      "Epoch   20/20 Batch 2/3 Cost: 1.014165\n",
      "Epoch   20/20 Batch 3/3 Cost: 0.091242\n"
     ]
    }
   ],
   "source": [
    "# 커스텀 데이터셋으로 선형 회귀 구현\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Dataset 상속\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.x_data = [[73, 80, 75],\n",
    "                   [93, 88, 93],\n",
    "                   [89, 91, 90],\n",
    "                   [96, 98, 100],\n",
    "                   [73, 66, 70]]\n",
    "        self.y_data = [[152], [185], [180], [196], [142]]\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"총 데이터의 개수 리턴\"\"\"\n",
    "        return len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"인덱스를 입력받아 맵핑되는 입출력 데이터를 파이토치의 텐서 형태로 리턴\"\"\"\n",
    "        x = torch.FloatTensor(self.x_data[idx])\n",
    "        y = torch.FloatTensor(self.y_data[idx])\n",
    "        return x, y\n",
    "\n",
    "dataset = CustomDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "model = torch.nn.Linear(3, 1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    for batch_idx, samples in enumerate(dataloader):\n",
    "        x_train, y_train = samples\n",
    "        prediction = model(x_train)\n",
    "        cost = F.mse_loss(prediction, y_train)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, batch_idx+1, len(dataloader), cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 후 입력이 73, 80, 75일 때의 예측값 : tensor([[152.1972]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 임의의 입력 [73, 80, 75]를 선언\n",
    "new_var =  torch.FloatTensor([[73, 80, 75]]) \n",
    "# 입력한 값 [73, 80, 75]에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n",
    "pred_y = model(new_var) \n",
    "print(\"훈련 후 입력이 73, 80, 75일 때의 예측값 :\", pred_y) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
