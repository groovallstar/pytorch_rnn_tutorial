{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ë‹¨ì¼ ë°°ì¹˜ ê¸°ë°˜ìœ¼ë¡œ ì‹œí€€ìŠ¤ ë¶„ë¥˜ê¸° í•™ìŠµ ë°©ë²•\n",
    "import torch\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\",\n",
    "             \"This course is amazing!\"]\n",
    "batch = tokenizer(sequences, padding=True, truncation=True, return_tensors='pt')\n",
    "batch['labels'] = torch.tensor([1, 1])\n",
    "\n",
    "optimizer = AdamW(model.parameters())\n",
    "loss = model(**batch).loss\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3938e1146b5041189f91f77a5dabe798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# í—ˆë¸Œì—ì„œ ë°ì´í„°ì…‹ ë¡œë”©\n",
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset('glue', 'mrpc')\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .', 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .', 'label': 1, 'idx': 0}\n",
      "{'sentence1': Value(dtype='string', id=None), 'sentence2': Value(dtype='string', id=None), 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None), 'idx': Value(dtype='int32', id=None)}\n"
     ]
    }
   ],
   "source": [
    "raw_train_dataset = raw_datasets['train']\n",
    "print(raw_train_dataset[0])\n",
    "\n",
    "# ì–´ë–¤ ì •ìˆ˜ê°€ ì–´ë–¤ ë ˆì´ë¸”ì¸ì§€ í™•ì¸í•  ë•ŒëŠ” features ê°’ í™•ì¸\n",
    "print(raw_train_dataset.features)\n",
    "\n",
    "# labelì€ ClassLabel íƒ€ì…ì´ê³  ë ˆì´ë¸” ì´ë¦„ì— ëŒ€í•œ ì •ìˆ˜ ë§¤í•‘ì€ names í´ë”ì— ì €ì¥ë˜ì–´ ìˆìŒ\n",
    "# 0ì€ not_equivalent, 1ì€ equivalent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë°ì´í„°ì…‹ ì „ì²˜ë¦¬  \n",
    "- í…ìŠ¤íŠ¸ë¥¼ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ìˆ«ìë¡œ ë³€í™˜í•´ì•¼ í•¨\n",
    "- í† í¬ë‚˜ì´ì €ì— ë‹¨ì¼ ë¬¸ì¥ ë˜ëŠ” ë‹¤ì¤‘ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë‹¤ìŒê³¼ ê°™ì´ ê° ìŒì˜ ëª¨ë“  ì²« ë²ˆì§¸ ë¬¸ì¥ê³¼ ë‘ ë²ˆì§¸ ë¬¸ì¥ì„ ê°ê° ì§ì ‘ í† í°í™”í•  ìˆ˜ ìˆìŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenized_sentences_1 = tokenizer(raw_datasets['train']['sentence1'])\n",
    "tokenized_sentences_2 = tokenizer(raw_datasets['train']['sentence2'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ë‘ ê°œì˜ ì‹œí€€ìŠ¤ë¥¼ ëª¨ë¸ì— ì „ë‹¬í•˜ì—¬ ë‘ ë¬¸ì¥ì´ ì˜ì—­ì¸ì§€ ì•„ë‹Œì§€ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ë°”ë¡œ ì–»ì„ ìˆ˜ ì—†ìŒ\n",
    "- ë‘ ì‹œí€€ìŠ¤ë¥¼ ìŒ(pair)ìœ¼ë¡œ ì²˜ë¦¬(ë‹¨ì¼ ë§¤ê°œë³€ìˆ˜ë¡œ ì²˜ë¦¬)í•˜ê³  ì ì ˆí•œ ì „ì²˜ë¦¬ë¥¼ ì ìš©í•´ì•¼ í•¨\n",
    "- tokenizerëŠ” í•œ ìŒì˜ ì‹œí€€ìŠ¤ë¥¼ ê°€ì ¸ì™€ BERT ëª¨ë¸ì´ ìš”êµ¬í•˜ëŠ” ì…ë ¥ í˜•íƒœë¡œ êµ¬ì„±í•  ìˆ˜ ìˆìŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer('This is the first sentence.', 'This is the second one.')\n",
    "print(inputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ìœ„ ì½”ë“œì—ì„œ ë§Œì¼ ì²«ë²ˆì§¸, ë‘ë²ˆì§¸ ë§¤ê°œë³€ìˆ˜ê°€ ë‹¤ì¤‘ ë¬¸ìì—´ì´ ë‹´ê¸´ ë¦¬ìŠ¤íŠ¸ë¼ë©´ ê° ë¦¬ìŠ¤íŠ¸ì— ì €ì¥ëœ ë¬¸ìì—´(ë¬¸ì¥)ì˜ ìˆœì„œëŒ€ë¡œ í•œ ìŒì”© í† í°í™”ë¨\n",
    "- ì²«ë²ˆì§¸ ë¦¬ìŠ¤íŠ¸ì˜ ì²«ë²ˆì§¸ ë¬¸ìì—´ê³¼ ë‘ë²ˆì§¸ ë¦¬ìŠ¤íŠ¸ì˜ ì²«ë²ˆì§¸ ë¬¸ìì—´ì´ í•˜ë‚˜ì˜ ë¬¸ìì—´ ìŒìœ¼ë¡œ ì…ë ¥ë˜ëŠ” ë°©ì‹\n",
    "- token_type_idsëŠ” ì „ì²´ ì…ë ¥(input_ids)ì˜ ì–´ëŠ ë¶€ë¶„ì´ ì²« ë²ˆì§¸ ë¬¸ì¥ì´ê³  ì–´ëŠ ê²ƒì´ ë‘ ë²ˆì§¸ ë¬¸ì¥ì¸ì§€ ëª¨ë¸ì— ì•Œë ¤ì¤Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# input_ids ë‚´ë¶€ì˜ IDë“¤ì„ ë‹¤ì‹œ ë‹¨ì–´ë¡œ ë””ì½”ë”©í•˜ë©´ ì•„ë˜ì™€ ê°™ìŒ\n",
    "# ëª¨ë¸ ì…ì¥ì—ì„œ ì…ë ¥ê°’ì´ \"[CLS] ë¬¸ì¥1 [SEP] ë¬¸ì¥2 [SEP]\"ì™€ ê°™ì´ ë  ê²ƒìœ¼ë¡œ ì•Œ ìˆ˜ ìˆìŒ\n",
    "print(tokenizer.convert_ids_to_tokens(inputs['input_ids']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"[CLS] ë¬¸ì¥1 [SEP]\"ì— ë¶€ë¶„ì€ token_type_idê°€ 0, \"ë¬¸ì¥2 [SEP]\"ì— í•´ë‹¹í•˜ëŠ” ë¶€ë¶„ì€ 1ì„\n",
    "- ë‹¤ë¥¸ checkpointë¥¼ ì„ íƒí•œë‹¤ë©´ í† í°í™”ëœ ì…ë ¥(tokenized inputs)ì— token_type_idsê°€ ì¡´ì¬í•˜ì§€ ì•Šì„ ìˆ˜ë„ ìˆìŒ\n",
    "  - DistilBERT ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ì—ëŠ” tokenizerê°€ token_type_idsë¥¼ ë°˜í™˜í•˜ì§€ ì•ŠìŒ\n",
    "  - ëª¨ë¸ì´ ì‚¬ì „í•™ìŠµ ê³¼ì •ì—ì„œ ì´ëŸ¬í•œ í˜•íƒœì˜ ì…ë ¥ í˜•ì‹ìœ¼ë¡œ í•™ìŠµì„ ì§„í–‰í–ˆì„ ê²½ìš°ì—ë§Œ ë°˜í™˜ë¨\n",
    "- ì‚¬ì „ í•™ìŠµ ê³¼ì •ì—ì„œ ë‹¤ìŒ ë¬¸ì¥ ì˜ˆì¸¡(next sentence prediction)ì„ ì‚¬ìš©í•˜ë©´ ëª¨ë¸ì— ë¬´ì‘ìœ„ë¡œ ë§ˆìŠ¤í‚¹ëœ í† í°(masked tokens)ì´ í¬í•¨ëœ ë¬¸ì¥ ìŒì´ ì…ë ¥ë˜ê³  ë‘ ë²ˆì§¸ ë¬¸ì¥ì´ ì²« ë²ˆì§¸ ë¬¸ì¥ì„ ë”°ë¥´ëŠ”ì§€ ì—¬ë¶€ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ ìš”êµ¬ë¨\n",
    "- í•™ìŠµ ê³¼ì •ì—ì„œ next sentence predictionì„ ì–´ë µì§€ ì•Šê²Œ í•˜ê¸° ìœ„í•´ ì…ë ¥ì˜ ì•½ 50%ëŠ” ë‘ ë¬¸ì¥ì´ ì›ë³¸ ë¬¸ì„œì—ì„œ ì—°ì†ì ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ” ìŒ ì§‘í•©ì´ë©° ë‚˜ë¨¸ì§€ 50%ëŠ” ë¬¸ì¥ ìŒì„ ì„œë¡œ ë‹¤ë¥¸ ë¬¸ì„œì—ì„œ ì¶”ì¶œëœ ë¬¸ì¥ë“¤ë¡œ êµ¬ì„±í•¨\n",
    "- í† í°í™” ì™„ë£Œëœ ì…ë ¥ì— token_type_idsê°€ ìˆëŠ”ì§€ ì—¬ë¶€ì— ëŒ€í•´ ê±±ì •í•  í•„ìš”ê°€ ì—†ìŒ\n",
    "  - í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ëª¨ë‘ì— ë™ì¼í•œ checkpointë¥¼ ì‚¬ìš©í•˜ë©´ í† í¬ë‚˜ì´ì €ëŠ” ëª¨ë¸ì— ë¬´ì—‡ì„ ì œê³µí•´ì•¼ í•˜ëŠ”ì§€ ì•Œê³  ìˆê¸° ë•Œë¬¸ì— ë¬¸ì œê°€ ì—†ìŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ ë°ì´í„°ì…‹ì„ ì „ì²˜ë¦¬í•˜ëŠ” í•˜ë‚˜ì˜ ë°©ë²•\n",
    "tokenized_dataset = tokenizer(\n",
    "    raw_datasets['train']['sentence1'], raw_datasets['train']['sentence2'],\n",
    "    padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - íŠ¹ì • ë°ì´í„°ë¥¼ dataset ê°ì²´ë¡œ ìœ ì§€í•˜ê¸° ìœ„í•´ Dataset.map() ë©”ì„œë“œë¥¼ ì‚¬ìš©í•¨\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['sentence1'], example['sentence2'], truncation=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ìœ„ í•¨ìˆ˜ëŠ” ë°ì´í„°ì…‹ì˜ ê°œë³„ í•­ëª©ì´ ë‹´ê²¨ì§„ ë”•ì…”ë„ˆë¦¬ë¥¼ ì…ë ¥ë°›ì•„ì„œ input_ids, attention_mask ë° token_type_ids í‚¤ê°€ ì§€ì •ëœ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•¨ \n",
    "- tokenizerëŠ” ë¬¸ì¥ ìŒ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì‘ë™í•˜ê¸° ë•Œë¬¸ì— exampleì— ì—¬ëŸ¬ ìƒ˜í”Œì´ í¬í•¨ëœ ê²½ìš°ì—ë„ ì‘ë™í•¨ \n",
    "  - map()ì—ì„œ batched=True ì˜µì…˜ì„ ì‚¬ìš©í•  ìˆ˜ ìˆì–´ í† í°í™” ì†ë„ê°€ ë¹¨ë¼ì§\n",
    "- ëª¨ë“  ìƒ˜í”Œë“¤ì„ ìµœëŒ€ ê¸¸ì´ë¡œ ì±„ìš°ëŠ” ê²ƒì´ íš¨ìœ¨ì ì´ì§€ ì•Šê¸° ë•Œë¬¸ì— ë°°ì¹˜(batch) í˜•íƒœë¡œ ì‹¤í–‰í•  ë•Œ ì±„ì›€\n",
    "  - ì „ì²´ ë°ì´í„°ì…‹ì—ì„œì˜ ìµœëŒ€ ê¸¸ì´ê°€ ì•„ë‹ˆë¼ í•´ë‹¹ batch ë‚´ì—ì„œì˜ ìµœëŒ€ ê¸¸ì´ë¡œ ì±„ìš°ê¸°ë§Œ í•˜ë©´ ë˜ê¸° ë•Œë¬¸\n",
    "  - ì…ë ¥ì˜ ê¸¸ì´ê°€ ë§¤ìš° ê°€ë³€ì ì¼ ë•Œ ì‹œê°„ê³¼ ì²˜ë¦¬ ëŠ¥ë ¥ ì ˆì•½ ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-3dde15d73cdbba3e.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-aeb1355e0d6fe019.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-4b5667fe9e3cdb60.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# í•œ ë²ˆì— ëª¨ë“  ë°ì´í„°ì…‹ì— í† í°í™”ë¥¼ ì ìš©í•˜ëŠ” ë°©ë²•\n",
    "# mapë©”ì„œë“œì—ì„œ batched=Trueë¥¼ ì‚¬ìš©í•˜ë©´ ê° batchì˜ ëª¨ë“  ìš”ì†Œë“¤ì— í•œêº¼ë²ˆì— ì ìš©ë¨\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” datasetsì— ìƒˆë¡œìš´ í•„ë“œë“¤ì„ ì¶”ê°€í•¨\n",
    "- ì´ í•„ë“œë“¤ì€ ì „ì²˜ë¦¬ í•¨ìˆ˜ì—ì„œ ë°˜í™˜ëœ ì‚¬ì „ì˜ ê° í‚¤(input_ids, token_type_ids, attention_mask)ê°’\n",
    "- num_proc ë§¤ê°œë³€ìˆ˜ë¥¼ ì „ë‹¬í•˜ì—¬ map()ìœ¼ë¡œ ì „ì²˜ë¦¬ ê¸°ëŠ¥ì„ ì ìš©í•  ë•Œ multi-processingì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ\n",
    "- ìœ„ì˜ tokenize_functionì€ input_ids, attention_mask, token_type_ids í‚¤ê°€ ì¡´ì¬í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ ì´ 3ê°œì˜ ìƒˆë¡œìš´ í•„ë“œê°€ ë°ì´í„°ì…‹ì˜ ëª¨ë“  ë¶„í• (í•™ìŠµ, ê²€ì¦, í‰ê°€)ì— ì¶”ê°€ë¨\n",
    "- ì „ì²˜ë¦¬ í•¨ìˆ˜ê°€ map()ì„ ì ìš©í•œ ë°ì´í„°ì…‹ì˜ ê¸°ì¡´ idx, label í‚¤ ë“±ì— ëŒ€í•œ ìƒˆë¡œìš´ ê°’ì„ ë°˜í™˜í•œ ê²½ìš° ê¸°ì¡´ í•„ë“œ(idx, label, sentence1, sentence2 ë“±)ë¥¼ ë³€ê²½í•  ìˆ˜ ìˆìŒ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë™ì  íŒ¨ë”©(Dynamic padding)  \n",
    "- ìƒ˜í”Œì„ í•¨ê»˜ ëª¨ì•„ ì§€ì •ëœ í¬ê¸°ì˜ batchë¡œ êµ¬ì„±í•˜ëŠ” ì—­í• ì„ í•˜ëŠ” í•¨ìˆ˜ë¥¼ collate functionì´ë¼ê³  í•¨\n",
    "- ì´ í•¨ìˆ˜ëŠ” DataLoaderë¥¼ buildí•  ë•Œ ì „ë‹¬í•  ìˆ˜ ìˆëŠ” ë§¤ê°œë³€ìˆ˜\n",
    "  - ê¸°ë³¸ê°’ì€ ìƒ˜í”Œë“¤ì„ PyTorch í…ì„œë¡œ ë³€í™˜í•˜ê³  ê²°í•©í•˜ëŠ” í•¨ìˆ˜\n",
    "  - ë§Œì¼ ëŒ€ìƒ ìƒ˜í”Œë“¤ì´ ë¦¬ìŠ¤íŠ¸, íŠœí”Œ, ë”•ì…”ë„ˆë¦¬ë©´ ì¬ê·€ì ìœ¼ë¡œ ì´ ì‘ì—…ì´ ìˆ˜í–‰ë¨\n",
    "  - ì˜ˆì œì˜ ê²½ìš° ì…ë ¥ê°’ì´ ëª¨ë‘ ë™ì¼í•œ ê¸¸ì´ê°€ ì•„ë‹ˆê¸° ë•Œë¬¸ì— ì´ ì‘ì—…ì´ ë¶ˆê°€ëŠ¥í•¨\n",
    "- ì „ì²´ ë°ì´í„°ì…‹ì´ ì•„ë‹Œ ê°œë³„ batchì— ëŒ€í•´ ë³„ë„ paddingì„ ìˆ˜í–‰í•˜ì—¬ ê³¼ë„í•˜ê²Œ ê¸´ ì…ë ¥ìœ¼ë¡œ ì¸í•œ ê³¼ë„í•œ padding ì‘ì—…ì„ ë°©ì§€í•¨\n",
    "- ì‹¤ì œë¡œ ì´ë¥¼ ìˆ˜í–‰í•˜ë ¤ë©´ batchë¡œ ë¶„ë¦¬í•˜ë ¤ëŠ” ë°ì´í„°ì…‹ì˜ ìš”ì†Œì— ëŒ€í•´ì„œ ì •í™•í•œ ìˆ˜ì˜ íŒ¨ë”©(padding)ì„ ì ìš©í•  ìˆ˜ ìˆëŠ” collate functionì„ ì •ì˜í•´ì•¼ í•¨\n",
    "  - Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ DataCollatorWithPaddingì„ í†µí•´ ë¨\n",
    "  - ì´ í•¨ìˆ˜ëŠ” í† í¬ë‚˜ì´ì €ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ìŒ. ì‚¬ìš©í•˜ë ¤ëŠ” íŒ¨ë”© í† í°(padding token)ì´ ë¬´ì—‡ì¸ì§€ì™€ ëª¨ë¸ì´ ì…ë ¥ì˜ ì™¼ìª½ í˜¹ì€ ì˜¤ë¥¸ì¯• ì¤‘ ì–´ëŠ ìª½ì— íŒ¨ë”©(padding)ì„ ìˆ˜í–‰í• ì§€ë¥¼ íŒŒì•…í•˜ê¸° ìœ„í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50, 59, 47, 67, 59, 50, 62, 32]\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# í•™ìŠµë°ì´í„°ì—ì„œ batchë¡œ ë¬¶ì„ ëª‡ê°œì˜ ìƒ˜í”Œë¡œ í…ŒìŠ¤íŠ¸\n",
    "samples = tokenized_datasets[\"train\"][:8]\n",
    "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
    "\n",
    "# 32ì—ì„œ 67ê¹Œì§€ ë‹¤ì–‘í•œ ê¸¸ì´ì˜ ìƒ˜í”Œì„ ì–»ì„ ìˆ˜ ìˆìŒ\n",
    "# ë™ì  íŒ¨ë”©ì€ ì´ batch ë‚´ì˜ ëª¨ë“  ìƒ˜í”Œë“¤ì´ ë°°ì¹˜ ë‚´ë¶€ì—ì„œ ìµœëŒ€ ê¸¸ì´ì¸ 67 ê¸¸ì´ë¡œ \n",
    "# padding ë˜ì–´ì•¼ í•¨. dynamic paddingì´ ì—†ìœ¼ë©´ ëª¨ë“  ìƒ˜í”Œë“¤ì€ ì „ì²´ ë°ì´í„°ì…‹ì˜ ìµœëŒ€ \n",
    "# ê¸¸ì´ ë˜ëŠ” ëª¨ë¸ì´ í—ˆìš©í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ ê¸¸ì´ë¡œ ì±„ì›Œì ¸ì•¼ í•¨\n",
    "print([len(x) for x in samples[\"input_ids\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([8, 67]),\n",
       " 'token_type_ids': torch.Size([8, 67]),\n",
       " 'attention_mask': torch.Size([8, 67]),\n",
       " 'labels': torch.Size([8])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë™ì ìœ¼ë¡œ batchë¥¼ padding í•˜ëŠ”ì§€ í™•ì¸\n",
    "batch = data_collator(samples)\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer APIë¥¼ ì´ìš©í•œ ëª¨ë¸ ë¯¸ì„¸ ì¡°ì •(fine-tuning)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b98627c8f9ad410197ac02f05666cda0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-e25e0335d4cc5aef.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-01b1d4497b862966.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-c9ea211c9fd30c10.arrow\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset('glue', 'mrpc')\n",
    "checkpoint = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example['sentence1'], example['sentence2'], truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í•™ìŠµ (Training)  \n",
    "- ë¨¼ì € Trainerê°€ í•™ìŠµ ë° í‰ê°€ì— ì‚¬ìš©í•  í•˜ì´í¼íŒŒë¼ë¯¸í„°(hyperparameters)ë¥¼ í¬í•¨í•˜ëŠ” TrainingArguments í´ë˜ìŠ¤ë¥¼ ì •ì˜í•´ì•¼ í•¨\n",
    "- ì—¬ê¸°ì„œ ì œê³µí•´ì•¼ í•  ë§¤ê°œë³€ìˆ˜ëŠ” í•™ìŠµëœ ëª¨ë¸ì´ ì €ì¥ë  ë””ë ‰í† ë¦¬ ê°’\n",
    "  - ë‚˜ë¨¸ì§€ëŠ” ëª¨ë‘ ê¸°ë³¸ê°’(default values)ì„ í™œìš©í•´ë„ ë¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments('test-trainer')\n",
    "\n",
    "# ë‘ ê°œì˜ ë ˆì´ë¸”ì´ ìˆëŠ” AutoModelForSequenceClassification í´ë˜ìŠ¤ ì‚¬ìš©\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='690' max='690' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [690/690 01:52, Epoch 3.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.349600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=690, training_loss=0.2794818352961886, metrics={'train_runtime': 116.0851, 'train_samples_per_second': 94.793, 'train_steps_per_second': 5.944, 'total_flos': 430291408824720.0, 'train_loss': 0.2794818352961886, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trainer ì •ì˜\n",
    "from transformers import Trainer\n",
    "trainer = Trainer(\n",
    "    model, training_args, \n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    data_collator=data_collator, # ì´ì „ì— ì •ì˜ëœ DataCollatorWithPadding\n",
    "    tokenizer=tokenizer,)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 500ë‹¨ê³„ë§ˆë‹¤ í•™ìŠµ ì†ì‹¤(training loss)ì´ ë³´ê³ ë˜ì§€ë§Œ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì¢‹ì€ì§€ í˜¹ì€ ë‚˜ìœì§€ëŠ” ì•Œë ¤ì£¼ì§€ ì•ŠìŒ\n",
    "  - í•™ìŠµ ê³¼ì •ì—ì„œ evaluation_strategyë¥¼ \"steps\"(ë§¤ eval_stepsë§ˆë‹¤ í‰ê°€)ë‚˜ \"epoch\"(ê° epoch ë§ˆì§€ë§‰ì— í‰ê°€) ë“±ìœ¼ë¡œ ì§€ì •í•˜ì§€ ì•Šì•˜ìŒ\n",
    "  - í‰ê°€ ë°©ë²• í˜¹ì€ í‰ê°€ ì²™ë„ë¥¼ ì •ì˜í•œ compute_metrics() í•¨ìˆ˜ë¥¼ Trainerì— ì§€ì •í•˜ì§€ ì•Šì•˜ìŒ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í‰ê°€ (Evaluation)  \n",
    "- compute_metrics()ë¥¼ êµ¬í˜„í•˜ê³  í•™ìŠµí•  ë•Œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•\n",
    "  - ì´ í•¨ìˆ˜ëŠ” EvalPrediction ê°ì²´(predictions í•„ë“œì™€ label_ids í•„ë“œê°€ í¬í•¨ëœ ë„¤ì„ë“œíŠœí”Œ(named tuple))ë¥¼ í•„ìš”ë¡œ í•˜ë©° ë¬¸ìì—´ê³¼ ì‹¤ìˆ˜ê°’(floats)ì„ ë§¤í•‘í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•¨\n",
    "  - ì—¬ê¸°ì„œ ë¬¸ìì—´ì€ ë°˜í™˜ëœ ë©”íŠ¸ë¦­(metrics)ì˜ ì´ë¦„ì´ê³  ì‹¤ìˆ˜ê°’(floats)ì€ í•´ë‹¹ ë©”íŠ¸ë¦­ì— ê¸°ë°˜í•œ í‰ê°€ ê²°ê³¼ê°’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24/26 00:01 < 00:00, 17.13 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(408, 2) (408,)\n"
     ]
    }
   ],
   "source": [
    "# ëª¨ë¸ì—ì„œ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì–»ì„ ë•Œ ì“°ëŠ” method\n",
    "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "print(predictions.predictions.shape, predictions.label_ids.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- predict()ëŠ” predictions, label_ids, metricsê°€ ìˆëŠ” ë„¤ì„ë“œíŠœí”Œ(named tuple)ê°’\n",
    "- metrics í•„ë“œì—ëŠ” ì „ë‹¬ëœ ë°ì´í„°ì…‹ì˜ lossì™€ ì‹œê°„ ë©”íŠ¸ë¦­(time metrics) ê°’ë§Œ í¬í•¨ë¨\n",
    "- ì‹œê°„ ë©”íŠ¸ë¦­(time metrics)ì€ ì˜ˆì¸¡ì— ê±¸ë¦° ì „ì²´ ë° í‰ê·  ì‹œê°„\n",
    "- compute_metrics() í•¨ìˆ˜ë¥¼ êµ¬ì„±í•˜ê³  Trainerì— ì „ë‹¬í•˜ë©´ í•´ë‹¹ í•„ë“œì—ëŠ” compute_metrics()ì—ì„œ ë°˜í™˜í•œ ë©”íŠ¸ë¦­(metrics)ë„ í¬í•¨ë¨\n",
    "- predictionsì€ 408 x 2ì¸ 2ì°¨ì› ë°°ì—´\n",
    "  - 408 ê°œëŠ” ìš°ë¦¬ê°€ ì˜ˆì¸¡ì— ì‚¬ìš©í•œ ë°ì´í„°ì…‹ì˜ ê°œìˆ˜\n",
    "  - í•´ë‹¹ ê°’ì€ predict()ì— ì „ë‹¬í•œ ë°ì´í„°ì…‹ì˜ ê° ìš”ì†Œì— ëŒ€í•œ ë¡œì§“(logit)ê°’\n",
    "  - ëª¨ë“  Transformer ëª¨ë¸ì€ ë¡œì§“(logit)ê°’ì„ ë°˜í™˜í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì´ ë¡œì§“(logit)ê°’ë“¤ì„ ë ˆì´ë¸”ê³¼ ë¹„êµí•  ìˆ˜ ìˆëŠ” ì˜ˆì¸¡ ê²°ê³¼ë¡œ ë³€í™˜í•˜ë ¤ë©´ \n",
    "# ë‘ ë²ˆì§¸ ì¶•ì— ì¡´ì¬í•˜ëŠ” í•­ëª©ì—ì„œ ìµœëŒ€ê°’ì´ ìˆëŠ” ì¸ë±ìŠ¤ë¥¼ ê°€ì ¸ì™€ì•¼ í•¨\n",
    "import numpy as np\n",
    "preds = np.argmax(predictions.predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25530/1271866207.py:6: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"glue\", \"mrpc\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8799019607843137, 'f1': 0.9144851657940662}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute_metric() í•¨ìˆ˜ë¥¼ êµ¬ì„±í•˜ê¸° ìœ„í•´ Datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©\n",
    "# load_metric() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ MRPC ë°ì´í„°ì…‹ê³¼ ê´€ë ¨ëœ metricì„ ë¡œë“œí•¨\n",
    "# ë¡œë“œëœ ê°ì²´ì—ëŠ” ë©”íŠ¸ë¦­(metrics) ê³„ì‚°ì„ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” compute() ë©”ì„œë“œê°€ ìˆìŒ\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"glue\", \"mrpc\")\n",
    "# ëª¨ë¸ í—¤ë“œë¥¼ ë¬´ì‘ìœ„ë¡œ ì´ˆê¸°í™”í•˜ë©´ ê³„ì‚°ëœ ë©”íŠ¸ë¦­ì´ ë³€ê²½ë  ìˆ˜ ìˆì–´ ì •í™•í•œ ê²°ê³¼ëŠ” ë‹¤ë¥¼ ìˆ˜ ìˆìŒ\n",
    "metric.compute(predictions=preds, references=predictions.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í‰ê°€ í•¨ìˆ˜ ì •ë¦¬\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = load_metric(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='691' max='690' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [690/690 01:57, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.421852</td>\n",
       "      <td>0.835784</td>\n",
       "      <td>0.889984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.375808</td>\n",
       "      <td>0.845588</td>\n",
       "      <td>0.893761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/26 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=690, training_loss=0.32166418545487996, metrics={'train_runtime': 121.0357, 'train_samples_per_second': 90.915, 'train_steps_per_second': 5.701, 'total_flos': 430291408824720.0, 'train_loss': 0.32166418545487996, 'epoch': 3.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ê° epochê°€ ëë‚  ë•Œ ë©”íŠ¸ë¦­(metrics)ì„ ì¶œë ¥í•˜ë„ë¡ í•˜ê¸° ìœ„í•´ \n",
    "# compute_metrics() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒˆ Trainerë¥¼ ì •ì˜í•˜ëŠ” ë°©ë²•\n",
    "training_args = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "trainer = Trainer(\n",
    "    model, training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# evaluation_strategy ê°’ì„ \"epoch\"ìœ¼ë¡œ ì„¤ì •í•˜ê³  ìƒˆë¡œìš´ TrainingArguments ë° ëª¨ë¸ ìƒì„±\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì „ì²´ í•™ìŠµ (Full Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "622d38ede17e457dbfe9dc54d4d93b6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-cb0e5869fff58506.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-3f2266ca56c4f876.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-3d7bd5692b533052.arrow\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í•™ìŠµì„ ìœ„í•œ ì¤€ë¹„  \n",
    "- í•™ìŠµ ë£¨í”„(training loop)ë¥¼ ì‘ì„±í•˜ê¸° ì „ì— ëª‡ ê°€ì§€ ê°ì²´ë¥¼ ì •ì˜í•´ì•¼ í•¨\n",
    "  - ë°°ì¹˜(batch)ë¥¼ ë°˜ë³µí•˜ëŠ” ë° ì‚¬ìš©í•  dataloaders ê°ì²´\n",
    "  - ì´ dataloadersë¥¼ ì •ì˜í•˜ê¸° ì „ì— tokenized_datasetsì— í›„ì²˜ë¦¬ë¥¼ ì ìš©í•´ì•¼ í•¨\n",
    "    - ëª¨ë¸ì´ í•„ìš”ë¡œ í•˜ì§€ ì•ŠëŠ” ê°’ì´ ì €ì¥ëœ columns ì œê±°\n",
    "    - column labelì˜ ì´ë¦„ì„ labelsë¡œ ë°”ê¿”ì•¼ í•¨. ì´ëŠ” ëª¨ë¸ì´ labelsë¼ëŠ” ì´ë¦„ìœ¼ë¡œ ë§¤ê°œë³€ìˆ˜ë¥¼ ë°›ê¸° ë•Œë¬¸.\n",
    "    - íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ ëŒ€ì‹  PyTorch í…ì„œ(tensors)ë¥¼ ë°˜í™˜í•˜ë„ë¡ datasetsì˜ í˜•ì‹ì„ ì„¤ì • í•´ì•¼ í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labels', 'input_ids', 'token_type_ids', 'attention_mask']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenized_datasetsì— ì´ëŸ° ì‘ì—…ì„ ìœ„í•œ ë³„ë„ì˜ ë©”ì„œë“œ ì¡´ì¬\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìœ„ ì½”ë“œë¡œ ì¸í•´ ê²°ê³¼ì ìœ¼ë¡œ tokenized_datasetsì—ëŠ” \n",
    "# ëª¨ë¸ì´ í—ˆìš©í•˜ëŠ” columnsë§Œ ì¡´ì¬í•¨ì„ ì•Œ ìˆ˜ ìˆìŒ\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator,)\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], batch_size=8, collate_fn=data_collator,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'labels': torch.Size([8]),\n",
       " 'input_ids': torch.Size([8, 68]),\n",
       " 'token_type_ids': torch.Size([8, 68]),\n",
       " 'attention_mask': torch.Size([8, 68])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë°ì´í„° ì²˜ë¦¬ì— ì˜¤ë¥˜ í™•ì¸ì„ ìœ„í•´ batch ì²´í¬\n",
    "for batch in train_dataloader:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}\n",
    "\n",
    "# ì‹¤ì œ ë°ì´í„° shape ì´ ë‹¤ë¥¼ ìˆ˜ ìˆëŠ”ë° ì´ëŠ” í•™ìŠµ dataloaderì— ëŒ€í•´ shuffle=Trueë¥¼ \n",
    "# ì„¤ì •í•˜ê³  ë°°ì¹˜(batch) ë‚´ì—ì„œì˜ ìµœëŒ€ ê¸¸ì´ë¡œ íŒ¨ë”©(padding)í•˜ê¸° ë•Œë¬¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6796, grad_fn=<NllLossBackward0>) torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "# ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤í™”\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "# batch ì „ë‹¬\n",
    "outputs = model(**batch)\n",
    "\n",
    "# ëª¨ë“  Transformers ëª¨ë¸ì€ ë§¤ê°œë³€ìˆ˜ì— labelsì´ í¬í•¨ë˜ì–´ ìˆë‹¤ë©´ ì†ì‹¤(loss)ê³¼ í•¨ê»˜ \n",
    "# logitê°’ë„ ë°˜í™˜í•¨ (batch ë‚´ì˜ logitê°’ì´ 2ê°œì´ë¯€ë¡œ 8 x 2ì¸ í…ì„œ)\n",
    "print(outputs.loss, outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ìµœì í™” í•¨ìˆ˜(optimizer) ì§€ì •\n",
    "# weight decay regularization ì ìš©\n",
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë§ˆì§€ë§‰ìœ¼ë¡œ, Trainerì—ì„œ ë””í´íŠ¸ë¡œ ì‚¬ìš©ë˜ëŠ” í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬(learning rate scheduler)ëŠ” ìµœëŒ€ê°’(5e-5)ì—ì„œ 0ê¹Œì§€ ì„ í˜• ê°ì‡ (linear decay)í•©ë‹ˆë‹¤. ì´ë¥¼ ì ì ˆí•˜ê²Œ ì •ì˜í•˜ë ¤ë©´ ìš°ë¦¬ê°€ ìˆ˜í–‰í•  í•™ìŠµ ë‹¨ê³„ì˜ íšŸìˆ˜ë¥¼ ì•Œì•„ì•¼ í•©ë‹ˆë‹¤. ì´ëŠ” ì‹¤í–‰í•˜ë ¤ëŠ” ì—í¬í¬(epochs) ìˆ˜ì— í•™ìŠµ ë°°ì¹˜(batch)ì˜ ê°œìˆ˜ë¥¼ ê³±í•œ ê²ƒì…ë‹ˆë‹¤. í•™ìŠµ ë°°ì¹˜ì˜ ê°œìˆ˜ëŠ” í•™ìŠµ dataloaderì˜ ê¸¸ì´ì™€ ê°™ìŠµë‹ˆë‹¤. TrainerëŠ” ë””í´íŠ¸ë¡œ 3ê°œì˜ ì—í¬í¬(epochs)ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ë‹¤ìŒì„ ë”°ë¦…ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1377\n"
     ]
    }
   ],
   "source": [
    "# í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬(learning rate scheduler)\n",
    "# ìµœëŒ€ê°’(5e-5)ì—ì„œ 0ê¹Œì§€ ì„ í˜• ê°ì‡ (linear decay)í•¨. \n",
    "# ì´ë¥¼ ì ì ˆí•˜ê²Œ ì •ì˜í•˜ë ¤ë©´ ìˆ˜í–‰í•  í•™ìŠµ ë‹¨ê³„ì˜ íšŸìˆ˜ë¥¼ ì•Œì•„ì•¼ í•¨.\n",
    "# -> ì‹¤í–‰í•˜ë ¤ëŠ” ì—í¬í¬(epochs) ìˆ˜ì— í•™ìŠµ ë°°ì¹˜(batch)ì˜ ê°œìˆ˜ë¥¼ ê³±í•œ ê²ƒ\n",
    "# í•™ìŠµ ë°°ì¹˜ ê°œìˆ˜ = train dataloader length\n",
    "from transformers import get_scheduler\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\", optimizer=optimizer, num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,)\n",
    "print(num_training_steps)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í•™ìŠµ ë£¨í”„ (Training Loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4646b394933e4e00aaa42b6b1fc5baf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í‰ê°€ ë£¨í”„ (Evaluation Loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8480392156862745, 'f1': 0.8931034482758621}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"glue\", \"mrpc\")\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    # metric.add_batch() ë©”ì„œë“œë¡œ í‰ê°€ ë£¨í”„(evaluation loop)ë¥¼ ì‹¤í–‰í•˜ë©´ì„œ \n",
    "    # ë°°ì¹˜(batch)ë³„ í‰ê°€ ë©”íŠ¸ë¦­(metrics) ê³„ì‚° ê²°ê³¼ë¥¼ ëˆ„ì í•  ìˆ˜ ìˆìŒ\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "# ëª¨ë“  ë°°ì¹˜(batch)ë¥¼ ëˆ„ì í•˜ê³  ë‚˜ë©´ metric.compute()ë¡œ ìµœì¢… ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒ\n",
    "metric.compute()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accelerateë¥¼ ì‚¬ìš©í•œ í•™ìŠµ ë£¨í”„ ê°€ì†í™”  \n",
    "- Accelerate ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ GPU ë˜ëŠ” TPUì—ì„œ ë¶„ì‚° í•™ìŠµ(distributed training)ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "041a07f2fb1e4770a9bbd3a54e941919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n",
    "\n",
    "# ë¶„ì‚° ì„¤ì •ì„ ì´ˆê¸°í™”í•˜ëŠ” Accelerator ê°ì²´ ì´ˆê¸°í™”\n",
    "# ì¥ì¹˜ ë°°ì¹˜(device placement)ë¥¼ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•˜ë¯€ë¡œ ì¥ì¹˜ì— ëª¨ë¸ì„ ë°°ì¹˜í•˜ëŠ” \n",
    "# model.to(device)ì„ ì œê±°í•  ìˆ˜ ìˆìŒ.\n",
    "# í•„ìš”í•œ ê²½ìš° device ëŒ€ì‹  accelerator.deviceë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# ì œê±°\n",
    "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "# model ë° optimizerë¥¼ ì…ë ¥í•¨\n",
    "# ì…ë ¥í•œ ê°ì²´ë“¤ì„ ì ì ˆí•œ ì»¨í…Œì´ë„ˆë¡œ ê°ì‹¸ì„œ ë¶„ì‚° í•™ìŠµì´ ì˜ë„ëŒ€ë¡œ ì‘ë™ë˜ë„ë¡ í•¨\n",
    "train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(\n",
    "    train_dataloader, eval_dataloader, model, optimizer)\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\", optimizer=optimizer, num_warmup_steps=0, \n",
    "    num_training_steps=num_training_steps)\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        # batch = {k: v.to(device) for k, v in batch.items()} ì œê±°\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss) # loss.backward ëŒ€ì²´\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì „ì²´ ì½”ë“œ ì •ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92c04cd83e8e407aa674e61b9bca59b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-cb0e5869fff58506.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-3f2266ca56c4f876.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-3d7bd5692b533052.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['labels', 'input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e71872e58ee42e58d60c7b1f8b2db3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/tmp/ipykernel_29121/115193445.py:88: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"glue\", \"mrpc\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8480392156862745, 'f1': 0.8916083916083916}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from transformers import AutoModelForSequenceClassification, AdamW, get_scheduler\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ë°ì´í„° ì…‹ ì ì¬\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "# ì‚¬ì „í•™ìŠµ ì–¸ì–´ëª¨ë¸ checkpoint ì´ë¦„ ì§€ì •\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "# ì§€ì •ëœ ì‚¬ì „í•™ìŠµ ì–¸ì–´ëª¨ë¸ì—ì„œ í† í¬ë‚˜ì´ì € ì¸ìŠ¤í„´ìŠ¤í™”\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € í•¨ìˆ˜ ì‚¬ìš©ì ì •ì˜í™” (sentence1, sentence2 ì»¬ëŸ¼ì— ëŒ€í•´ì„œë§Œ í† í¬ë‚˜ì´ì§• ìˆ˜í–‰)\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "# í† í¬ë‚˜ì´ì§• ìˆ˜í–‰\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "# ë°°ì¹˜(batch)ë³„ íŒ¨ë”©(padding)ì„ ìœ„í•œ data collator ì •ì˜\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# ë¶ˆí•„ìš”í•œ ì…ë ¥ ì»¬ëŸ¼ì„ ì œê±°í•˜ê³  ì‚¬ì „í•™ìŠµ ì–¸ì–´ëª¨ë¸ì— í•„ìš”í•œ ì…ë ¥ë§Œ ë‚¨ê¹€.\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(\n",
    "    [\"sentence1\", \"sentence2\", \"idx\"])\n",
    "# ë°ì´í„°ì…‹ì˜ label ì»¬ëŸ¼ëª…ì„ labelsë¡œ ë³€ê²½\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "# ë°ì´í„°ì…‹ì˜ ìœ í˜•ì„ PyTorch tensorë¡œ ë³€ê²½\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# ë³€ê²½ëœ ì»¬ëŸ¼ ì¶œë ¥\n",
    "print(tokenized_datasets[\"train\"].column_names)\n",
    "\n",
    "\n",
    "# ê° ì¢…ë¥˜ë³„ ë°ì´í„° ë¡œë” ìƒì„±\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator)\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], shuffle=True, batch_size=8,\n",
    "    collate_fn=data_collator)\n",
    "\n",
    "# ì‚¬ì „í•™ìŠµ ì–¸ì–´ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤í™”\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint, num_labels=2)\n",
    "# ìµœì í™” í•¨ìˆ˜ ì •ì˜\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# ì—í¬í¬ ê°œìˆ˜ ì„¤ì •\n",
    "num_epochs = 3\n",
    "# í•™ìŠµ ìŠ¤í… ìˆ˜ ê³„ì‚°\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "# í•™ìŠµ ìŠ¤ì¼€ì¥´ëŸ¬ ì„¤ì •\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\", optimizer=optimizer, num_warmup_steps=0, \n",
    "    num_training_steps=num_training_steps)\n",
    "\n",
    "# GPUë¡œ ëª¨ë¸ì„ ì´ë™\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# ì§„í–‰ ìƒí™©ë°” ì •ì˜\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "# ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ì „í™˜\n",
    "model.train()\n",
    "# í•™ìŠµ ë£¨í”„ ì‹œì‘\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        # í˜„ì¬ ë°°ì¹˜ ì¤‘ì—ì„œ ì…ë ¥ê°’ì„ ëª¨ë‘ GPUë¡œ ì´ë™.\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        # ëª¨ë¸ ì‹¤í–‰\n",
    "        outputs = model(**batch)\n",
    "        # ì†ì‹¤ê°’ ê°€ì ¸ì˜¤ê¸°\n",
    "        loss = outputs.loss\n",
    "        # ì—­ì „íŒŒ ìˆ˜í–‰\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "# í‰ê°€ ë©”íŠ¸ë¦­ ê°€ì ¸ì˜¤ê¸°\n",
    "metric = load_metric(\"glue\", \"mrpc\")\n",
    "# ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì „í™˜\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "# í‰ê°€ ê²°ê³¼ ê³„ì‚° ë° ì¶œë ¥ \n",
    "metric.compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
