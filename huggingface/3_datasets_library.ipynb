{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로컬 데이터셋 로딩하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-05-25 16:15:01--  https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz\n",
      "Resolving github.com (github.com)... 20.200.245.247\n",
      "Connecting to github.com (github.com)|20.200.245.247|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/crux82/squad-it/master/SQuAD_it-train.json.gz [following]\n",
      "--2023-05-25 16:15:02--  https://raw.githubusercontent.com/crux82/squad-it/master/SQuAD_it-train.json.gz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7725286 (7.4M) [application/octet-stream]\n",
      "Saving to: ‘SQuAD_it-train.json.gz’\n",
      "\n",
      "SQuAD_it-train.json 100%[===================>]   7.37M  19.4MB/s    in 0.4s    \n",
      "\n",
      "2023-05-25 16:15:03 (19.4 MB/s) - ‘SQuAD_it-train.json.gz’ saved [7725286/7725286]\n",
      "\n",
      "--2023-05-25 16:15:03--  https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz\n",
      "Resolving github.com (github.com)... 20.200.245.247\n",
      "Connecting to github.com (github.com)|20.200.245.247|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/crux82/squad-it/master/SQuAD_it-test.json.gz [following]\n",
      "--2023-05-25 16:15:03--  https://raw.githubusercontent.com/crux82/squad-it/master/SQuAD_it-test.json.gz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1051245 (1.0M) [application/octet-stream]\n",
      "Saving to: ‘SQuAD_it-test.json.gz’\n",
      "\n",
      "SQuAD_it-test.json. 100%[===================>]   1.00M  4.76MB/s    in 0.2s    \n",
      "\n",
      "2023-05-25 16:15:04 (4.76 MB/s) - ‘SQuAD_it-test.json.gz’ saved [1051245/1051245]\n",
      "\n",
      "SQuAD_it-test.json.gz:\t 87.5% -- replaced with SQuAD_it-test.json\n",
      "SQuAD_it-train.json.gz:\t 82.3% -- replaced with SQuAD_it-train.json\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz\n",
    "!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz\n",
    "!gzip -dkv SQuAD_it*.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-14841bacf120a6eb/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9e6ba0fc8bd41888fa96ad6b9ce0275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10ef1384815b4c1bbd73406bb5eeac3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61196f37ee3943a19b614c52cd70f944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-14841bacf120a6eb/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e24cce412fef4894a9cdb652e5d1405e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['paragraphs', 'title'],\n",
      "        num_rows: 442\n",
      "    })\n",
      "})\n",
      "{'paragraphs': [{'context': 'La frase \"51° stato\" può essere usata in senso positivo, nel senso che una regione o territorio è così allineata, solidale e favorevole agli Stati Uniti che è come uno stato americano. Può anche essere utilizzato in senso peggiorativo, il che significa che un\\' area o una regione è percepita come sottoposta a un\\' eccessiva influenza o controllo militare o culturale americano. In vari paesi del mondo, le persone che credono che la loro cultura locale o nazionale sia troppo americanizzata usano talvolta il termine \"51° stato\" in riferimento al proprio paese. eccessiva influenza o controllo culturale o militare americano eccessivo.', 'qas': [{'answers': [{'answer_start': 71, 'text': 'una regione o territorio è così allineata, solidale e favorevole agli Stati Uniti'}], 'id': '572eedffc246551400ce47bc', 'question': 'Qual è la connotazione positiva dell\\' etichetta \"51° stato\"?'}]}, {'context': 'L\\'articolo 4, III sezione della Costituzione degli Stati Uniti prevede il potere del Congresso di annettere nuovi stati all\\'Unione. Per entrare nell\\'Unione uno stato deve dare «\"full faith and credit\"» (\"piena fiducia e credito\") agli atti di tutte le assemblee legislative e di tutti i tribunali, compresi i contratti legali, i matrimoni e le sentenze penali. Di contro uno Stato annesso all\\'Unione godrebbe della protezione militare e civile del Governo federale, che è anche tenuto a garantire una forma di governo repubblicana ad ogni Stato dell\\'Unione.', 'qas': []}, {'context': \"Puerto Rico è stato discusso come potenziale 51° stato degli Stati Uniti. In un referendum sullo status del 2012 la maggioranza degli elettori, il 54%, ha espresso insoddisfazione per l' attuale relazione politica. In una domanda separata, il 61 per cento degli elettori ha sostenuto la statualità (escludendo il 26 per cento degli elettori che hanno lasciato in bianco l' interrogazione). L' 11 dicembre 2012, il legislatore di Porto Rico ha deciso di chiedere che il Presidente e il Congresso degli Stati Uniti agiscano sui risultati, mettano fine all' attuale forma di status territoriale e avviino il processo di ammissione di Porto Rico all' Unione come stato. Puerto Rico è stato discusso come potenziale 51° stato degli Stati Uniti.\", 'qas': [{'answers': [{'answer_start': 147, 'text': '54%'}], 'id': '572ef128c246551400ce47d5', 'question': 'Quale percentuale di elettori ha sostenuto la statualità?'}, {'answers': [{'answer_start': 393, 'text': '11 dicembre 2012'}], 'id': '572ef128c246551400ce47d6', 'question': 'Quando ha deciso Porto Rico di chiedere agli Stati Uniti di agire su questi risultati?'}, {'answers': [{'answer_start': 429, 'text': 'Porto Rico'}], 'id': '572ef128c246551400ce47d8', 'question': 'Quale territorio è stato proposto come nuova aggiunta agli Stati Uniti?'}, {'answers': [{'answer_start': 147, 'text': '54%'}], 'id': '572efe86cb0c0d14000f16de', 'question': \"Quali percentuali di elettori hanno espresso infelicità per l' attuale rapporto politico tra Stati Uniti e Porto Rico?\"}, {'answers': [{'answer_start': 393, 'text': '11 dicembre 2012'}], 'id': '572efe86cb0c0d14000f16df', 'question': 'Quando ha chiesto al legislatore di Puerto Rico di essere considerato Stato?'}]}, {'context': \"Dal 1898, Puerto Rico ha una rappresentanza limitata al Congresso sotto forma di Commissario Residente, un delegato senza diritto di voto. Il 110° Congresso ha restituito al Commissario il potere di votare in seno al Comitato dell' intero, ma non su questioni in cui il voto rappresenterebbe una partecipazione decisiva. Puerto Rico ha le elezioni presidenziali degli Stati Uniti presidenziali primari o caucus del Partito Democratico e il Partito Repubblicano di selezionare i delegati alle rispettive convenzioni nazionali dei partiti, anche se gli elettori presidenziali non sono concessi sul collegio elettorale. Come cittadini americani, i portoricani possono votare alle elezioni presidenziali negli Stati Uniti, a condizione che risiedano in uno dei 50 stati o nel Distretto di Columbia e non nello stesso Porto Rico. Dal 1898.\", 'qas': [{'answers': [{'answer_start': 142, 'text': '110° Congresso'}], 'id': '572eff62dfa6aa1500f8d540', 'question': 'Quale Congresso ha dato al commissario il potere di votare?'}]}, {'context': \"I residenti di Puerto Rico pagano imposte federali statunitensi: tasse di import/export, tasse federali sulle materie prime, imposte sociali, contribuendo così al governo americano. La maggior parte dei residenti di Puerto Rico non pagano l' imposta federale sul reddito, ma pagano le imposte federali sui salari (Social Security e Medicare). Tuttavia, i dipendenti federali, quelli che fanno affari con il governo federale, Puerto Rico società con sede a Rico che intendono inviare fondi per gli Stati Uniti e altri fanno pagare le imposte federali sul reddito. Puerto Ricans può arruolarsi nei militari statunitensi. I portoricani hanno partecipato a tutte le guerre americane dal 1898;52 portoricani erano stati uccisi nella guerra in Iraq e in Afghanistan nel novembre 2012. Imposte federali statunitensi. Quali tipi di tasse pagano i portoricani? tasse di importazione/esportazione, tasse federali sulle materie prime, imposte sociali. Che cosa costituisce le tasse federali Puerto Rican cittadini ricani pagare? Puerto Ricans può arruolarsi nel militare degli Stati Uniti.\", 'qas': []}, {'context': \"Puerto Rico è stato sotto la sovranità degli Stati Uniti per oltre un secolo quando è stato ceduto agli Stati Uniti dalla Spagna dopo la fine della guerra ispano-americana, e Puerto Ricans sono stati cittadini statunitensi dal 1917. Lo status definitivo dell' isola non è stato determinato a partire dal 2012[aggiornamento], i suoi residenti non hanno la rappresentanza di voto nel loro governo federale. Puerto Rico ha una rappresentanza limitata al Congresso degli Stati Uniti sotto forma di Commissario Residente, un delegato senza diritto di voto limitato. Come gli stati, Puerto Rico ha l' autogoverno, una forma repubblicana di governo organizzata secondo una costituzione adottata dal suo popolo, e una polizza di diritti. Guerra Spagnolo-Americana. All' indomani di quale guerra gli Stati Uniti hanno concesso Puerto Rico? Spagna. Quale paese ha consegnato Portorico agli Stati Uniti? dal 1917. Da quanto tempo i portoricani sono cittadini statunitensi? repubblicano.\", 'qas': []}, {'context': \"Nel novembre 2012, un referendum ha portato il 54 per cento degli intervistati a votare per rifiutare lo status attuale ai sensi della clausola territoriale della Costituzione degli Stati Uniti, mentre una seconda domanda ha portato al 61 per cento degli elettori identificando la statualità come l' alternativa preferita allo status territoriale attuale. Il referendum del 2012 è stato di gran lunga il referendum di maggior successo per i sostenitori della statualità e il sostegno allo Stato è aumentato in ogni referendum popolare successivo. Tuttavia, più di un elettore su quattro si è astenuto dal rispondere alla domanda sullo status alternativo preferito. Gli oppositori dello Stato hanno sostenuto che l' opzione della statualità ha raccolto solo il 45 per cento dei voti se sono incluse le astensioni. Se si considerano le astensioni, il risultato del referendum è molto più vicino al 44 per cento per la statualità, un numero che cade sotto la soglia della maggioranza del 50 per cento. 54 per cento.\", 'qas': [{'answers': [{'answer_start': 564, 'text': 'un elettore su quattro'}], 'id': '5730429d04bcaa1900d77438', 'question': 'Quale percentuale degli elettori si è astenuta dal voto su uno status alternativo preferito?'}]}, {'context': 'Il Washington Post, il New York Times e il Boston Herald hanno pubblicato pareri che esprimono il loro sostegno alla statualità di Porto Rico. L\\' 8 novembre 2012, Washington, il quotidiano The Hill ha pubblicato un articolo in cui si afferma che il Congresso probabilmente ignorerà i risultati del referendum a causa delle circostanze che stanno alla base dei voti. e Stati Uniti d\\' America Il membro del Congresso Luis Gutiérrez U. S. U. Nydia Velázquez, entrambi di origini portoricane, si è dichiarato d\\' accordo con le dichiarazioni di The Hill. Poco dopo la pubblicazione dei risultati, Puerto Rico-nato negli Stati Uniti. Il membro del Congresso José Enrique Serrano ha commentato:\"Sono rimasto particolarmente colpito dall\\' esito del referendum sullo\" status \"tenutosi a Porto Rico. La maggioranza dei votanti ha segnalato la volontà di modificare l\\' attuale assetto territoriale. In una seconda domanda, una maggioranza ancora più ampia ha chiesto di diventare Stato. Questo è un terremoto nella politica di Puerto Rican. Richiederà l\\' attenzione del Congresso, e una risposta definitiva alla richiesta di cambiamento del Porto Rican. Questo è un momento storico in cui gli elettori hanno chiesto di andare avanti\".', 'qas': [{'answers': [{'answer_start': 189, 'text': 'The Hill'}], 'id': '57304372a23a5019007fd020', 'question': 'Che cosa ha suggerito che il Congresso avrebbe ignorato il referendum di Puerto Rico?'}, {'answers': [{'answer_start': 0, 'text': 'Il Washington Post, il New York Times e il Boston Herald'}], 'id': '57304372a23a5019007fd01f', 'question': 'Quali giornali hanno pubblicato articoli di opinione che esprimono il loro sostegno alla statualità di Puerto Rico?'}, {'answers': [{'answer_start': 146, 'text': '8 novembre 2012'}], 'id': '57304372a23a5019007fd021', 'question': \"Quando è stato pubblicato l' articolo?\"}]}, {'context': 'Alcuni giorni dopo il referendum, il Commissario Pedro Pierluisi, Governatore Luis Fortuño, e il Governatore eletto Alejandro García Padilla hanno scritto al Presidente degli Stati Uniti Barack Obama lettere separate sui risultati delle votazioni. Pierluisi ha esortato Obama ad iniziare la legislazione a favore della statualità di Puerto Rico, alla luce della sua vittoria nel referendum. Fortuño lo esortò a portare avanti il processo. García Padilla gli chiese di respingere i risultati a causa della loro ambiguità. La posizione della Casa Bianca relativa al plebiscito del novembre 2012 era che i risultati erano chiari, il popolo di Puerto Rico vuole la questione dello status risolto, e una maggioranza ha scelto lo stato nella seconda domanda. L\\' ex direttore della Casa Bianca dei media ispanici ha dichiarato:\"Ora è ora che il Congresso agisca e l\\' amministrazione lavorerà con loro su questo sforzo, in modo che il popolo di Puerto Rico possa determinare il proprio futuro\".', 'qas': [{'answers': [{'answer_start': 37, 'text': 'Commissario Pedro Pierluisi, Governatore Luis Fortuño'}], 'id': '573044ce947a6a140053d386', 'question': 'Quali politici hanno spinto per la statualità di Puerto Rico?'}, {'answers': [{'answer_start': 97, 'text': 'Governatore eletto Alejandro García Padilla'}], 'id': '573044ce947a6a140053d387', 'question': 'Quale politico ha combattuto contro la statualità per Puerto Rico?'}]}, {'context': 'Il 15 maggio 2013, il Commissario Pierluisi ha presentato H. R. 2000 al Congresso per \"definire il processo per l\\' ammissione di Porto Rico come stato dell\\' Unione\", chiedendo al Congresso di votare sulla ratifica di Porto Rico come 51° stato. Il 12 febbraio 2014, il senatore Martin Heinrich ha presentato un disegno di legge al Senato degli Stati Uniti. Il progetto di legge richiederebbe l\\' indizione di un referendum vincolante a Portorico per chiedere se il territorio vuole essere ammesso come Stato.', 'qas': []}, {'context': 'Washington, DC è spesso citato come candidato alla statualità. In Federalista No. 43 di The Federalist Papers, James Madison ha considerato le implicazioni della definizione di \"sede del governo\" contenuta nella Costituzione degli Stati Uniti. Anche se ha notato potenziali conflitti di interesse, e la necessità di una \"legislatura comunale per scopi locali\", Madison non ha affrontato il ruolo del distretto nel voto nazionale. Gli studiosi di diritto non sono d\\' accordo sul fatto che un semplice atto del Congresso possa ammettere il Distretto come Stato, a causa del suo status di sede del governo degli Stati Uniti, che l\\' Articolo I, Sezione 8 della Costituzione richiede di essere sotto la giurisdizione esclusiva del Congresso; a seconda dell\\' interpretazione di questo testo, l\\' ammissione del Distretto intero come Stato può richiedere un emendamento costituzionale, che è molto più difficile da mettere in pratica. Tuttavia, la Costituzione non stabilisce una dimensione minima per il Distretto. Le sue dimensioni sono già cambiate una volta, quando Virginia ha recuperato la porzione del Distretto a sud del Potomac. Così il requisito costituzionale per un distretto federale può essere soddisfatto riducendo le sue dimensioni al piccolo nucleo centrale di edifici governativi e monumenti, dando il resto del territorio al nuovo Stato. Giacomo Madison.', 'qas': [{'answers': [{'answer_start': 111, 'text': 'James Madison'}], 'id': '573047ca947a6a140053d396', 'question': 'Dove si trovano queste idee nei documenti federalisti?'}]}, {'context': 'Washington, D. C. residenti che sostengono il movimento statualitÃ\\xa0 a volte usano una versione abbreviata del motto di protesta della guerra rivoluzionaria \"nessuna tassazione senza rappresentanza\", omettendo l\\' iniziale \"No\", denotando la loro mancanza di rappresentanza congressuale, la frase Ã¨ ora stampata su recentemente pubblicato Washington, D. C. targhe (anche se un driver puÃ² scegliere di avere l\\' indirizzo del sito web Washington, DC invece). La limousine presidenziale del presidente Bill Clinton aveva la targa \"Taxation without representation\" in ritardo nel suo mandato, mentre il presidente George W. Bush aveva cambiato le targhe del veicolo poco dopo l\\' inizio del suo mandato.', 'qas': [{'answers': [{'answer_start': 237, 'text': 'la loro mancanza di rappresentanza congressuale'}], 'id': '57304a372461fd1900a9ccf4', 'question': 'Che cosa è la frase di protesta?'}]}, {'context': \"Questa posizione è stata assunta dal D. C. Statehood Party, un partito politico; da allora si è fusa con l' affiliata locale del partito verde per formare il D. C. Statehood Green Party. Il movimento più vicino a questo movimento è mai venuto al successo è stato nel 1978, quando il Congresso ha approvato il Distretto di Columbia voto Diritto di voto Emendamento. Due anni dopo, nel 1980, i cittadini locali approvarono un' iniziativa che chiedeva una convenzione costituzionale per un nuovo Stato. Nel 1982, gli elettori ratificarono la costituzione dello Stato, che doveva essere chiamato Nuova Columbia.\", 'qas': [{'answers': [{'answer_start': 267, 'text': '1978'}], 'id': '57304ae58ab72b1400f9c3f6', 'question': 'Quando il movimento si è avvicinato di più al successo?'}, {'answers': [{'answer_start': 592, 'text': 'Nuova Columbia'}], 'id': '57304ae58ab72b1400f9c3f7', 'question': 'Quale sarebbe il nuovo nome dello Stato DC?'}]}, {'context': 'Altri contendenti meno probabili sono Guam e le Isole Vergini degli Stati Uniti, entrambe non costituite in società, territori organizzati degli Stati Uniti. Inoltre, le isole Marianne settentrionali e Samoa americane, un territorio non sindacalizzato e senza personalità giuridica, potrebbero entrambi cercare di ottenere la sovranità. Alcune proposte prevedono l\\' ammissione delle Isole Vergini con Porto Rico come Stato unico (spesso conosciuto come il proposto \"Commonwealth di Prusvi\", per Porto Rico/USA). Isole Vergini, o come \"Puerto Virgo\"), e per l\\' amalgama dei territori degli Stati Uniti o ex territori nell\\' Oceano Pacifico, nel modo del concetto di \"Greater Hawaii\" degli anni Sessanta. Guam e le Isole Marianne del Nord sarebbero ammesse come un unico Stato, insieme a Palau, agli Stati Federati di Micronesia e alle Isole Marshall (anche se queste ultime tre entità sono ora nazioni sovrane separate, che hanno un patto di libero accordo di associazione con gli Stati Uniti). Tale stato avrebbe una popolazione di 412.381 abitanti (leggermente inferiore alla popolazione del Wyoming) e una superficie di 911,82 miglia quadrate (2.361,6 km2) (leggermente più piccola dell\\' isola di Rodi). Samoa americana potrebbe forse essere parte di tale stato, aumentando la popolazione a 467.900 e l\\' area a 988,65 miglia quadrate (2.560,6 km2). Radio Australia, alla fine di maggio 2008, ha lanciato segnali di Guam e delle Isole Marianne del Nord, tornando a diventare il 51° Stato. Guam.', 'qas': [{'answers': [{'answer_start': 38, 'text': 'Guam'}], 'id': '573071c5069b5314008320d7', 'question': 'Qual è un altro paese probabile per la statualità?'}, {'answers': [{'answer_start': 1350, 'text': 'Radio Australia'}], 'id': '573071c5069b5314008320d8', 'question': 'Quale entità ha riferito che Guam e le Isole Marianne del Nord sono tornate a farlo?'}]}, {'context': 'Le Filippine hanno avuto piccoli movimenti di base per la statualità degli Stati Uniti. Originariamente parte della piattaforma del Partito Progressivo, allora conosciuto come Federalista Partito, il partito caduto nel 1907, che ha coinciso con il cambiamento di nome. Non più tardi del 2004, il concetto che le Filippine diventino uno Stato statunitense è stato parte di una piattaforma politica nelle Filippine. I sostenitori di questo movimento includono i filippini che credono che la qualità della vita nelle Filippine sarebbe più alta e che ci sarebbe meno povertà se le Filippine fossero uno stato o un territorio americano. I sostenitori includono anche i filippini che avevano combattuto come membri delle forze armate degli Stati Uniti in varie guerre durante il periodo del Commonwealth. Filippine. Quale paese aveva un piccolo movimento di base per lo Stato americano? Federalista Partito.', 'qas': []}, {'context': 'In Canada,\"il 51esimo stato\" è una frase generalmente usata in modo tale da implicare che se si prende un certo corso politico, il destino del Canada sarà poco più di una parte degli Stati Uniti. Tra gli esempi si possono citare l\\' accordo di libero scambio Canada-Stati Uniti del 1988, il dibattito sulla creazione di un perimetro di difesa comune, e come potenziale conseguenza della mancata adozione di proposte volte a risolvere la questione della sovranità del Quebec, l\\' accordo di Charlottetown del 1992 e la legge sulla chiarezza del 1999.', 'qas': [{'answers': [{'answer_start': 281, 'text': '1988'}], 'id': '57307298396df9190009610b', 'question': \"Quando è stato approvato l' accordo di libero scambio Canada-US?\"}, {'answers': [{'answer_start': 506, 'text': '1992'}], 'id': '57307298396df9190009610c', 'question': \"Quando è stato firmato l' Accordo di Charlottetown?\"}, {'answers': [{'answer_start': 542, 'text': '1999'}], 'id': '57307298396df9190009610d', 'question': 'Quando è entrata in vigore la legge sulla chiarezza?'}]}, {'context': 'La frase è di solito usata nei dibattiti politici locali, nella scrittura polemica o in conversazioni private. E\\' raramente utilizzato dai politici stessi in un contesto pubblico, anche se in certi momenti della storia canadese i partiti politici hanno usato altre immagini altrettanto cariche. Nelle elezioni federali del 1988, i liberali hanno affermato che il proposto accordo di libero scambio equivaleva a un\\' acquisizione americana di Canada-significa, il partito ha pubblicato un annuncio in cui i conservatori progressisti (PC) strateghi, al momento dell\\' adozione dell\\' accordo, lentamente cancellato il confine Canada-USA da una mappa desktop del Nord America. Nel giro di pochi giorni, tuttavia, i PC risposero con un annuncio che riportava il confine con un marcatore permanente, come un annunciatore intonato \"Ecco dove tracciamo la linea\". Liberali.', 'qas': [{'answers': [{'answer_start': 822, 'text': '\"Ecco dove tracciamo la linea\"'}], 'id': '5730730a069b5314008320e5', 'question': 'Qual era lo slogan di questo annuncio?'}]}, {'context': \"L' implicazione ha una base storica e risale alla rottura dell' America britannica durante la Rivoluzione Americana. Le colonie che avevano confederato per formare gli Stati Uniti invasero il Canada (all' epoca un termine che alludeva specificamente alle province moderne del Quebec e dell' Ontario, che erano in mano britannica solo dal 1763) almeno due volte, nessuna delle quali riuscì a prendere il controllo del territorio. La prima invasione fu durante la Rivoluzione, nell' ipotesi che la presunta ostilità dei canadesi francofoni verso il dominio coloniale britannico, unita all' alleanza franco-americana, li avrebbe resi alleati naturali della causa americana; l' esercito continentale reclutò con successo due reggimenti canadesi per l' invasione. Il fallimento di quell' invasione costrinse i membri di quei reggimenti all' esilio, e si stabilirono soprattutto nello stato di New York. Gli articoli della Confederazione, scritti durante la Rivoluzione, includevano una disposizione che prevedeva che il Canada entrasse a far parte degli Stati Uniti, qualora decidesse di farlo, senza bisogno di richiedere il permesso degli Stati Uniti come farebbero altri stati. Gli Stati Uniti di nuovo invasero il Canada durante la guerra del 1812, ma questo sforzo fu reso più difficile a causa del gran numero di americani lealisti che erano fuggiti in quello che ora è Ontario e ancora resistito a unirsi alla repubblica. I Hunter Patriots nel 1830 e le incursioni Fenian dopo la guerra civile americana sono stati attacchi privati contro il Canada da parte degli Stati Uniti Diversi politici statunitensi nel 19 ° secolo ha parlato anche a favore di annettere il Canada. 1763. Quando ha atterrato Quebec in mani britanniche? 1763. Quando l' Ontario ha atterrato in mani britanniche? la guerra del 1812.\", 'qas': [{'answers': [{'answer_start': 338, 'text': '1763'}], 'id': '5730739e2461fd1900a9ce1d', 'question': 'Cosa hanno suggerito diversi politici statunitensi nel XIX secolo?'}]}, {'context': 'Alla fine degli anni Quaranta, durante gli ultimi giorni del dominio di Terranova (all\\' epoca una dominione-dipendenza nel Commonwealth e indipendente dal Canada), c\\' è stato un generale sostegno, anche se non maggioritario, per la Terranova a formare un\\' unione economica con gli Stati Uniti, grazie agli sforzi del Partito dell\\' Unione Economica e a significativi investimenti statunitensi in Terranova provenienti dagli Stati Uniti - Regno Unito. Alla fine il movimento fallì quando, in un referendum del 1948, gli elettori scelsero di confederare con il Canada (il Partito dell\\' Unione Economica sostenne un \"governo responsabile\" indipendente che avrebbero poi spinto verso i loro obiettivi). Alla fine degli anni\\' 40.', 'qas': []}, {'context': 'Negli Stati Uniti, il termine \"51° Stato\" applicato al Canada può servire a evidenziare le somiglianze e le strette relazioni tra Stati Uniti e Canada. A volte il termine è usato in modo denigratorio, destinato a deridere il Canada come un vicino di casa poco importante. Nelle elezioni generali del Quebec, 1989, il partito politico Parti 51 ha condotto 11 candidati su una piattaforma di Quebec seceding dal Canada per entrare a far parte degli Stati Uniti (con il suo leader, André Perron, affermando Quebec non potrebbe sopravvivere come nazione indipendente). Il partito ha attirato solo 3.846 voti in tutta la provincia, lo 0,11% dei voti totali espressi. In confronto, gli altri partiti a favore della sovranità del Quebec in quanto le elezioni hanno ottenuto il 40,16% (PQ) e l\\' 1,22% (NPDQ).', 'qas': [{'answers': [{'answer_start': 213, 'text': 'deridere il Canada come un vicino di casa poco importante'}], 'id': '573074888ab72b1400f9c4f9', 'question': 'Qual è la connotazione negativa del termine?'}, {'answers': [{'answer_start': 588, 'text': 'solo 3.846 voti'}], 'id': '573074888ab72b1400f9c4fa', 'question': \"Come hanno attirato nel 1989 i voti dell' uomo il Parti 51?\"}]}, {'context': \"A causa della vicinanza geografica dei paesi dell' America centrale agli Stati Uniti, che ha influenze militari, economiche e politiche potenti, ci sono stati diversi movimenti e proposte da parte degli Stati Uniti durante i secoli XIX e XX ad annettere alcune o tutte le repubbliche centroamericane (Costa Rica, El Salvador, Guatemala, Honduras con l' ex British-ruled Bay Islands, Nicaragua, Panama che aveva il territorio degli Stati Uniti governato Canal Zone dal 190. Tuttavia, gli Stati Uniti non hanno mai agito sulla base di queste proposte di alcuni politici statunitensi, alcuni dei quali non sono mai stati consegnati o considerati seriamente. Nel 2001, El Salvador ha adottato il dollaro statunitense come valuta, mentre Panama lo ha utilizzato per decenni a causa dei suoi legami con la Zona Canale. dal 1903 al 1979. Quando Panama ha avuto il Canal Zone Territory regolato dagli Stati Uniti? 2001. Quando ha adottato El Salvador come moneta il dollaro USA? dal 1981. Quando l' Honduras britannico è diventato Belize?.\", 'qas': []}, {'context': \"Cuba, come molti territori spagnoli, voleva liberarsi dalla Spagna. Un movimento a favore dell' indipendenza a Cuba è stato sostenuto dagli Stati Uniti, e i leader della guerriglia cubana volevano l' annessione agli Stati Uniti, ma il leader rivoluzionario cubano José Martí ha invocato la nazione cubana. Quando la nave da guerra statunitense Maine affondò nel porto dell' Avana, gli Stati Uniti accusarono la Spagna e la guerra ispano-americana scoppiò nel 1898. Dopo che gli Stati Uniti hanno vinto, la Spagna ha rinunciato alla rivendicazione di sovranità sui territori, compresa Cuba. Gli Stati Uniti amministravano Cuba come protettorato fino al 1902. Alcuni decenni dopo, nel 1959, il corrotto governo cubano di Fulgencio Batista, appoggiato dagli Stati Uniti, fu rovesciato da Fidel Castro. Castro ha installato un governo marxista-leninista alleato con l' Unione Sovietica, che da allora è al potere. Spagna. Di quale paese era Cuba un territorio? Maine. Quale corazzata affondò nel porto dell' Avana? 1959. Quando ha rovesciato il governo cubano Fidel Castro? Governo marxista-leninista.\", 'qas': []}], 'title': '51º stato'}\n"
     ]
    }
   ],
   "source": [
    "# load_dataset() 함수를 사용하여 JSON 파일을 로드하려면 일반적인 JSON 형식(파이썬의 \n",
    "# 중첩 딕셔너리와 유사)인지 JSON Lines(줄로 구분된 JSON)인지 알아야 함\n",
    "# SQuAD-it는 하나의 data 필드에 모든 텍스트가 저장된 중첩 형식(nested format)을 사용함\n",
    "# 이는 field 매개변수를 지정하여 데이터셋을 로드할 수 있음\n",
    "from datasets import load_dataset\n",
    "squad_it_dataset = load_dataset('json', data_files='SQuAD_it-train.json', field='data')\n",
    "\n",
    "# 로컬 파일을 로드하면 학습 분할(train split)된 DatasetDict 개체 생성됨\n",
    "# 학습 분할에 존재하는 데이터의 행의 수(num_rows)와 열의 이름(features)이 출력됨\n",
    "print(squad_it_dataset)\n",
    "\n",
    "# 학습 분할(train split) 인덱싱 후 내용 확인 가능\n",
    "print(squad_it_dataset[\"train\"][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-0988dc9a9cdcf355/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daf30b9d8a9a4288a4189044eee04708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c3c0e2559384a8dace46c5e8845f398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baec518143574cfa9e8d444466ca44b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aed6e7e66ea4aa68d64bca05d4cef14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-0988dc9a9cdcf355/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f693d724726345c7b7c3599af0fff843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['paragraphs', 'title'],\n",
       "        num_rows: 442\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['paragraphs', 'title'],\n",
       "        num_rows: 48\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단일 DatasetDict 객체에 학습과 테스트 분할을 포함시켜 한꺼번에 두 집합에 \n",
    "# Dataset.map() 함수를 적용하는 방법.\n",
    "# data_files에 각 split name을 해당 집합 파일명에 매핑하는 딕셔너리 지정\n",
    "data_files = {\"train\": \"SQuAD_it-train.json\", \"test\": \"SQuAD_it-test.json\"}\n",
    "squad_it_dataset = load_dataset(\"json\", data_files=data_files, field=\"data\")\n",
    "squad_it_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-034c4ba1fcfd5f33/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf25f677a22148f0b7e1e5a760633485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a4b003955ab490296854766d521e1d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c071f5ea9341ee839a74f0f4fc01dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a240c82ac3f4befbff6fab3abbdd4e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-034c4ba1fcfd5f33/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c056c15e6c914d5596a3019c49f3e6ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 입력 파일의 압축 해제를 자동 지원.\n",
    "# 자동 압축 해제는 ZIP 및 TAR과 같은 다른 압축 형식에도 적용됨\n",
    "data_files = {'train': 'SQuAD_it-train.json.gz', 'test': 'SQuAD_it-test.json.gz'}\n",
    "squad_it_dataset = load_dataset('json', data_files=data_files, field='data')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원격 데이터셋 로딩하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-bf56bcbccfaf6dd0/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3333f6b36b194445905d6ac4cf873517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GitHub에서 호스팅되는 SQuAD-it 데이터셋의 경우 data_files 매개변수에 \n",
    "# SQuAD_it-*.json.gz URL을 지정할 수 있음\n",
    "url = \"https://github.com/crux82/squad-it/raw/master/\"\n",
    "data_files = {'train': url + 'SQuAD_it-test.json.gz', \n",
    "              'test': url + 'SQuAD_it-test.json.gz'}\n",
    "\n",
    "# gz파일을 수동으로 다운로드하고 압축을 해제하는 단계를 생략할 수 있음\n",
    "squad_it_dataset = load_dataset('json', data_files=data_files, field='data')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터셋 슬라이싱(slicing)과 다이싱(dicing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-05-25 16:18:10--  https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 42989872 (41M) [application/x-httpd-php]\n",
      "Saving to: ‘drugsCom_raw.zip’\n",
      "\n",
      "drugsCom_raw.zip    100%[===================>]  41.00M  9.08MB/s    in 4.8s    \n",
      "\n",
      "2023-05-25 16:18:16 (8.59 MB/s) - ‘drugsCom_raw.zip’ saved [42989872/42989872]\n",
      "\n",
      "Archive:  drugsCom_raw.zip\n",
      "  inflating: drugsComTest_raw.tsv    \n",
      "  inflating: drugsComTrain_raw.tsv   \n"
     ]
    }
   ],
   "source": [
    "# UC Irvine Machine Learning Repository의 Drug Review Dataset 다운로드\n",
    "# 다양한 약물에 대한 환자 리뷰, 치료 상태 및 환자 만족도에 대한 별 10개 등급 포함\n",
    "\n",
    "!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\"\n",
    "!unzip drugsCom_raw.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-774518853761a459/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efd74f31c1e94e96a7d8f6695a4178c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-774518853761a459/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-fb0a619296aeebea.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': [87571, 178045, 80482],\n",
       " 'drugName': ['Naproxen', 'Duloxetine', 'Mobic'],\n",
       " 'condition': ['Gout, Acute', 'ibromyalgia', 'Inflammatory Conditions'],\n",
       " 'review': ['\"like the previous person mention, I&#039;m a strong believer of aleve, it works faster for my gout than the prescription meds I take. No more going to the doctor for refills.....Aleve works!\"',\n",
       "  '\"I have taken Cymbalta for about a year and a half for fibromyalgia pain. It is great\\r\\nas a pain reducer and an anti-depressant, however, the side effects outweighed \\r\\nany benefit I got from it. I had trouble with restlessness, being tired constantly,\\r\\ndizziness, dry mouth, numbness and tingling in my feet, and horrible sweating. I am\\r\\nbeing weaned off of it now. Went from 60 mg to 30mg and now to 15 mg. I will be\\r\\noff completely in about a week. The fibro pain is coming back, but I would rather deal with it than the side effects.\"',\n",
       "  '\"I have been taking Mobic for over a year with no side effects other than an elevated blood pressure.  I had severe knee and ankle pain which completely went away after taking Mobic.  I attempted to stop the medication however pain returned after a few days.\"'],\n",
       " 'rating': [9.0, 3.0, 10.0],\n",
       " 'date': ['September 2, 2015', 'November 7, 2011', 'June 5, 2013'],\n",
       " 'usefulCount': [36, 13, 128]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TSV는 구분 기호로 쉼표 대신 탭을 사용하는 CSV의 변형.\n",
    "# load_dataset() 함수에 구분 기호 매개변수(delimiter)를 지정\n",
    "from datasets import load_dataset\n",
    "data_files = {'train': 'drugsComTrain_raw.tsv', 'test': 'drugsComTest_raw.tsv'}\n",
    "drug_dataset = load_dataset('csv', data_files=data_files, delimiter='\\t')\n",
    "\n",
    "# 일부 무작위 샘플 확인.\n",
    "# Datasets에서 shuffle(), select()를 chaining하여 무작위 샘플을 가져올 수 있음\n",
    "drug_sample = drug_dataset['train'].shuffle(seed=42).select(range(1000))\n",
    "drug_sample[:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 재현성(reproducibility)을 위해 Dataset.shuffle()의 seed를 고정\n",
    "- Dataset.select()는 반복 가능 인덱스(iterable indices)를 입력해야 하므로 range(1000)을 전달\n",
    "- 해당 데이터 셋의 단점\n",
    "  - \"Unnamed: 0\" 컬럼(column)은 확실하지는 않지만 각 환자의 익명 ID(anonymized ID)처럼 보임\n",
    "  - \"condition\" 컬럼(column)에는 대문자와 소문자 레이블이 혼합되어 있음\n",
    "  - review의 길이는 다양하며 줄 구분 기호(\\r\\n)와 '와 같은 HTML 문자 코드 혼합되어 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-774518853761a459/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-fa448e2786d6c2e4.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-774518853761a459/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-766f35b7c3c7bd97.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-774518853761a459/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-139a90e0e05c004d.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-774518853761a459/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-3e4fe24a3a4d26cb.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
      "        num_rows: 161297\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],\n",
      "        num_rows: 53766\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['left ventricular dysfunction', 'adhd', 'birth control']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"Unnamed: 0\" 컬럼이 환자의 식별자(anonymous ID)라는 가정이 맞는지 테스트하기 위해 \n",
    "# Dataset.unique() 함수를 사용하여 ID의 개수가 해당 데이터셋 분할들(splits)의 행(row)의 \n",
    "# 수와 일치하는지 확인\n",
    "for split in drug_dataset.keys():\n",
    "    assert len(drug_dataset[split]) == len(drug_dataset[split].unique('Unnamed: 0'))\n",
    "    \n",
    "# 데이터셋 정제\n",
    "\n",
    "# 컬럼명 변경\n",
    "drug_dataset = drug_dataset.rename_column(\n",
    "    original_column_name='Unnamed: 0', new_column_name='patient_id')\n",
    "print(drug_dataset)\n",
    "\n",
    "# map()을 사용하여 모든 condition 레이블 정규화.\n",
    "# 각 split의 모든 행에 적용하기 위한 method 추가\n",
    "def lowercase_condition(example):\n",
    "    return {'condition': example['condition'].lower()}\n",
    "\n",
    "# condition 컬럼의 일부 항목에 None이 포함되어 error 발생함.\n",
    "# drug_dataset.map(lowercase_condition)\n",
    "\n",
    "# lambda 함수를 이용하여 map, filter 작업을 정의할 수 있음. None 항목 제거.\n",
    "drug_dataset = drug_dataset.filter(lambda x: x['condition'] is not None)\n",
    "\n",
    "# 다시 정규화\n",
    "drug_dataset = drug_dataset.map(lowercase_condition)\n",
    "drug_dataset['train']['condition'][:3]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "새로운 컬럼(column) 만들기  \n",
    "- 고객 리뷰를 다룰 때마다 각 리뷰의 단어 수를 확인하는 것이 좋음\n",
    "  - 리뷰는 \"Great!\"와 같은 한 단어이거나 수천 개의 단어로 구성된 에세이(full-blown essays)로 작성되었을 수도 있음\n",
    "  - 사용 사례에 따라 다르게 처리해야 함\n",
    "  - 각 리뷰의 단어 수를 계산하기 위해 각 텍스트를 공백(whitespace)으로 나누는 대략적인 휴리스틱(heuristics)을 사용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-774518853761a459/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-bc288479d484fae4.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-774518853761a459/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-eed5181860f66e0e.arrow\n",
      "Loading cached sorted indices for dataset at /root/.cache/huggingface/datasets/csv/default-774518853761a459/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-d1654f972aa506ef.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-774518853761a459/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-36b53cb25f995a80.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-774518853761a459/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-322cf1e6ad76d5dd.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-774518853761a459/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-df224b5d448d9c10.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-774518853761a459/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-63bd6a1e26a9f4c1.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'patient_id': 206461, 'drugName': 'Valsartan', 'condition': 'left ventricular dysfunction', 'review': '\"It has no side effect, I take it in combination of Bystolic 5 Mg and Fish Oil\"', 'rating': 9.0, 'date': 'May 20, 2012', 'usefulCount': 27, 'review_length': 17}\n",
      "{'patient_id': [111469, 13653, 53602], 'drugName': ['Ledipasvir / sofosbuvir', 'Amphetamine / dextroamphetamine', 'Alesse'], 'condition': ['hepatitis c', 'adhd', 'birth control'], 'review': ['\"Headache\"', '\"Great\"', '\"Awesome\"'], 'rating': [10.0, 10.0, 10.0], 'date': ['February 3, 2015', 'October 20, 2009', 'November 23, 2015'], 'usefulCount': [41, 3, 0], 'review_length': [1, 1, 1]}\n",
      "{'train': 138514, 'test': 46108}\n"
     ]
    }
   ],
   "source": [
    "# 리뷰의 단어 수를 계산하는 함수\n",
    "# 데이터셋의 column 이름과 다른 새로운 키(review_length)를 가진 딕셔너리 반환.\n",
    "# Dataset.map()에 전달되면 모든 행에 적용되어 review_length 열을 생성함\n",
    "def compute_review_length(example):\n",
    "    return {'review_length': len(example['review'].split())}\n",
    "\n",
    "drug_dataset = drug_dataset.map(compute_review_length)\n",
    "print(drug_dataset['train'][0])\n",
    "\n",
    "# sort()를 사용하여 extreme value(극단값) 확인\n",
    "print(drug_dataset['train'].sort('review_length')[:3])\n",
    "\n",
    "# 데이터셋에 새 열을 추가하는 다른 방법 : Dataset.add_column()\n",
    "# 추가하고자 하는 column을 Python 리스트 또는 NumPy 배열로 제공함\n",
    "\n",
    "# 30단어 미만으로 표현된 리뷰 제거\n",
    "drug_dataset = drug_dataset.filter(lambda x: x['review_length'] > 30)\n",
    "print(drug_dataset.num_rows) # 약 15% 제거\n",
    "\n",
    "# html 문자가 있을 경우 이스케이프 해제(unescape)\n",
    "# text = 'I&#039;m a transformer called BERT'\n",
    "# print(html.unescape(text)) # I'm a transformer called BERT\n",
    "\n",
    "import html\n",
    "drug_dataset = drug_dataset.map(lambda x: {'review': html.unescape(x['review'])})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "map() 메서드  \n",
    "- Dataset.map()의 batched가 True로 설정되면 호출되는 순간마다 여러 개로 구성된 하나의 배치(batch)가 한번에 map 함수에 입력됨\n",
    "- 배치 크기(batch size)는 별도 설정 가능. 디폴트값은 1000\n",
    "  - 예를 들어 모든 HTML 특수문자를 unescape하기 위해서 위에서 실행한 map 함수는 실행 속도가 느림\n",
    "  - list comprehension을 사용하여 동시에 처리할 수 있음\n",
    "- batched=True를 지정하면 함수는 데이터 집합의 필드가 포함된 딕셔너리를 받지만 각 값은 이제 단일 값이 아닌 값 목록이 됨\n",
    "- Dataset.map()의 반환 값은 데이터 집합에 업데이트하거나 추가하려는 필드가 포함된 딕셔너리와 값 목록으로 동일해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['review'], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-774518853761a459/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-ca6c8382d8aa8f20.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-774518853761a459/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-f80ef624e5b4a328.arrow\n"
     ]
    }
   ],
   "source": [
    "# batched=True를 사용하여 HTML 문자의 이스케이프를 해제하는 방법\n",
    "new_drug_dataset = drug_dataset.map(\n",
    "    lambda x: {'review': [html.unescape(o) for o in x['review']]}, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-774518853761a459/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-e1bf23110c3010de_*_of_00008.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-774518853761a459/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-9e68e02afcba4f9c_*_of_00008.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 751 ms, sys: 31 ms, total: 782 ms\n",
      "Wall time: 780 ms\n"
     ]
    }
   ],
   "source": [
    "# Dataset.map()의 자체 병렬화 기능\n",
    "from transformers import AutoTokenizer\n",
    "slow_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'bert-base-cased', use_fast=False)\n",
    "\n",
    "def slow_tokenize_function(examples):\n",
    "    return slow_tokenizer(examples['review'], truncation=True)\n",
    "\n",
    "# num_proc 값을 조정한다고 무조건 성능이 향상되는 것은 아님.\n",
    "# num_proc을 사용하여 처리 속도를 높이는 것은 일반적으로 사용 중인 함수가 자체적으로 \n",
    "# 다중 처리를 수행하지 않는 경우에만 좋은 아이디어라고 볼 수 있음.\n",
    "%time tokenized_dataset = drug_dataset.map(slow_tokenize_function, batched=True, num_proc=8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 일반적으로 기계 학습에서 하나의 example은 모델에 제공하는 feature의 집합으로 정의됨\n",
    "- 특정 상황에서 이러한 feature는 Dataset 내의 column 집합으로 표현되지만, 다른 맥락에서는 여러 개의 feature가 하나의 단일 example에서 추출되어 단일 column에 속하는 경우도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101, 107, 1422, 1488, 1110, 9079, 1194, 1117, 2223, 1989, 1104, 1130, 19972, 11083, 119, 1284, 1245, 4264, 1165, 1119, 1310, 1142, 1314, 1989, 117, 1165, 1119, 1408, 1781, 1103, 2439, 13753, 1119, 1209, 1129, 1113, 119, 1370, 1160, 1552, 117, 1119, 1180, 6374, 1243, 1149, 1104, 1908, 117, 1108, 1304, 172, 14687, 1183, 117, 1105, 7362, 1111, 2212, 129, 2005, 1113, 170, 2797, 1313, 1121, 1278, 12020, 113, 1304, 5283, 1111, 1140, 119, 114, 146, 1270, 1117, 3995, 1113, 6356, 2106, 1105, 1131, 1163, 1106, 6166, 1122, 1149, 170, 1374, 1552, 119, 3969, 1293, 1119, 1225, 1120, 1278, 117, 1105, 1114, 2033, 1146, 1107, 1103, 2106, 119, 1109, 1314, 1160, 1552, 1138, 1151, 2463, 1714, 119, 1124, 1110, 150, 21986, 3048, 1167, 5340, 1895, 1190, 1518, 102], [101, 119, 1124, 1110, 1750, 6438, 113, 170, 1363, 1645, 114, 117, 1750, 172, 14687, 1183, 119, 1124, 1110, 11566, 1155, 1103, 1614, 1119, 1431, 119, 8007, 1117, 4658, 1110, 1618, 119, 1284, 1138, 1793, 1242, 1472, 23897, 1105, 1177, 1677, 1142, 1110, 1103, 1211, 3903, 119, 107, 102]]\n"
     ]
    }
   ],
   "source": [
    "# examples을 토큰화하고 최대 길이 128로 자름\n",
    "\n",
    "# return_overflowing_tokens=True로 지정함으로써 토크나이저에게 전체 review의 앞부분에 있는 \n",
    "# 128개의 토큰으로 구성된 청크(chunk)가 아니라 모든 청크(chunk)를 반환하도록 요청함\n",
    "def tokenize_and_split(examples):\n",
    "    return tokenizer(examples['review'], truncation=True, \n",
    "                     max_length=128, return_overflowing_tokens=True,)\n",
    "    \n",
    "# 전체 데이터셋에 대해서 Dataset.map()을 사용하기 전에 테스트\n",
    "result = tokenize_and_split(drug_dataset['train'][0])\n",
    "[len(inp) for inp in result['input_ids']]\n",
    "print(result['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습집합의 첫 번째 example는 우리가 지정한 최대 토큰 수(128)보다 많이 토큰화되었기 \n",
    "# 때문에 두 가지 feature로 구성됨.\n",
    "# 첫번째 feature의 길이는 128이고 두번째 feature의 길이는 49.\n",
    "\n",
    "# error 발생\n",
    "# tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 오류가 발생한 이유는 컬럼(column) 중 하나의 개수에 불일치가 있음\n",
    "  - 하나는 개수가 1,463이고 다른 하나는 길이가 1,000\n",
    "  - Dataset.map()을 보면 이 1000은 매핑하는 함수(tokenize_and_split)에 전달된 총 샘플 개수\n",
    "  - 1,000개의 예제가 입력되어 1,463개의 새로운 feature들을 출력하므로 shape error가 발생함\n",
    "- 문제는 크기가 다른 두 개의 서로 다른 데이터셋을 혼합하려고 한다는 것\n",
    "- 이 오류에서 drug_dataset 열에는 1,000개의 예제가 있으나 새롭게 구성하려는 tokenized_dataset에는 그보다 많은 수의 예제(1,463)가 있음 Dataset 객체로서는 제대로 작동할 수가 없음\n",
    "- 이전 데이터셋의 열을 제거하거나 새로운 데이터셋과 동일한 크기로 만들어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-774518853761a459/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-b54bd56e181254fb.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-774518853761a459/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-af604e755f8ddb54.arrow\n"
     ]
    }
   ],
   "source": [
    "# remove_columns 매개변수 지정\n",
    "tokenized_dataset = drug_dataset.map(\n",
    "    tokenize_and_split, batched=True, remove_columns=drug_dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(206772, 138514)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 길이를 비교하여 새 데이터셋에 원래 데이터셋보다 많은 요소가 있는지 확인\n",
    "len(tokenized_dataset[\"train\"]), len(drug_dataset[\"train\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- return_overflowing_tokens=True를 설정할 때 토크나이저가 반환하는 overflow_to_sample_mapping 필드가 필요함\n",
    "- 이는 우리에게 새로운 feature 인덱스에서 그것이 시작된 샘플의 인덱스로의 매핑을 제공\n",
    "- 이를 사용하여 새로운 feature를 생성하는 횟수만큼 각 예제의 값을 반복하여 원본 데이터셋에 있는 각 키를 올바른 크기의 값 목록과 연결할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-774518853761a459/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-4035e7bd8eacd829.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-774518853761a459/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-45268106c7197148.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 206772\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 68876\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_split(examples):\n",
    "    result = tokenizer(examples['review'], truncation=True, max_length=128,\n",
    "                       return_overflowing_tokens=True)\n",
    "    # 신규 인덱스와 이전 인덱스와의 매핑 추출\n",
    "    sample_map = result.pop('overflow_to_sample_mapping')\n",
    "    for key, values in examples.items():\n",
    "        result[key] = [values[i] for i in sample_map]\n",
    "    return result\n",
    "\n",
    "tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   patient_id    drugName      condition  \\\n",
      "0       95260  Guanfacine           adhd   \n",
      "1       92703      Lybrel  birth control   \n",
      "2      138000  Ortho Evra  birth control   \n",
      "\n",
      "                                              review  rating  \\\n",
      "0  \"My son is halfway through his fourth week of ...     8.0   \n",
      "1  \"I used to take another oral contraceptive, wh...     5.0   \n",
      "2  \"This is my first time using any form of birth...     8.0   \n",
      "\n",
      "                date  usefulCount  review_length  \n",
      "0     April 27, 2010          192            141  \n",
      "1  December 14, 2009           17            134  \n",
      "2   November 3, 2015           10             89  \n",
      "       condition  frequency\n",
      "0  birth control      27655\n",
      "1     depression       8023\n",
      "2           acne       5209\n",
      "3        anxiety       4991\n",
      "4           pain       4744\n",
      "Dataset({\n",
      "    features: ['condition', 'frequency'],\n",
      "    num_rows: 819\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 서드 파티 라이브러리 간 변환을 위해 Dataset.set_format() 함수 제공\n",
    "drug_dataset.set_format(\"pandas\")\n",
    "\n",
    "# DataFrame 사용\n",
    "print(drug_dataset[\"train\"][:3])\n",
    "train_df = drug_dataset[\"train\"][:]\n",
    "\n",
    "frequencies = (\n",
    "    train_df[\"condition\"].value_counts().to_frame().reset_index().rename(\n",
    "        columns={\"index\": \"condition\", \"condition\": \"frequency\"}))\n",
    "print(frequencies.head())\n",
    "\n",
    "# from_pandas() 함수를 사용하여 Dataset 객체 생성\n",
    "from datasets import Dataset\n",
    "freq_dataset = Dataset.from_pandas(frequencies)\n",
    "print(freq_dataset)\n",
    "\n",
    "drug_dataset.reset_format()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 내부적으로 Dataset.set_format()은 데이터셋의 __getitem__() 던더(dunder, double under) 메서드에 대한 반환 형식을 변경함\n",
    "- pandas 형식의 Dataset에서 train_df와 같은 새 객체를 생성하려는 경우 pandas.DataFrame을 얻기 위해 전체 데이터셋을 슬라이싱(slicing)해야 함을 의미함\n",
    "- 출력 형식에 관계없이 drug_dataset[\"train\"]의 유형이 Dataset임을 직접 확인할 수 있음"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "검증 집합(validation set) 생성  \n",
    "- Datasets는 scikit-learn의 유명한 기능을 기반으로 하는 Dataset.train_test_split() 함수를 제공함\n",
    "- 이를 사용하여 학습 집합을 학습 및 검증 집합으로 분할함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/csv/default-774518853761a459/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-7cc7de6b96cc4523.arrow and /root/.cache/huggingface/datasets/csv/default-774518853761a459/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-c70faff307fa6037.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
      "        num_rows: 110811\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
      "        num_rows: 27703\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
      "        num_rows: 46108\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2fc3e8da4434d539ad60477388fb03a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/110811 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4bf607dfe794170b50c6736585048a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/27703 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b9b5703fdb4bf59fcde74b172485b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/46108 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
      "        num_rows: 110811\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
      "        num_rows: 27703\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],\n",
      "        num_rows: 46108\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "405c67f8159c47dea49ddff45d229259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/111 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c82bd97c1b9b448680728276bc4be5d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/28 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d1916b30074e16a70d595abcc55b75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/47 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-48fde84539a3dc73/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f60549380d714584952cd7ffccd09716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14828bec73e648d8a2d4038075f853ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "468ff4c11dac4c79bb410b0cbec9976e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4944bbdece304022806127ff9f1fe9af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbc8efeadaad481395055b519094ff38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-48fde84539a3dc73/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fa670fc886e4f9d830153bee7ad4ad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drug_dataset_clean = drug_dataset[\"train\"].train_test_split(train_size=0.8, seed=42)\n",
    "# 기본 test split을 validation으로 변경\n",
    "drug_dataset_clean[\"validation\"] = drug_dataset_clean.pop(\"test\")\n",
    "# test 추가\n",
    "drug_dataset_clean[\"test\"] = drug_dataset[\"test\"]\n",
    "print(drug_dataset_clean)\n",
    "\n",
    "# 데이터셋을 Arrow 형식으로 저장\n",
    "drug_dataset_clean.save_to_disk(\"drug-reviews\")\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# arrow 형식 파일 로드\n",
    "drug_dataset_reloaded = load_from_disk(\"drug-reviews\")\n",
    "print(drug_dataset_reloaded)\n",
    "\n",
    "# CSV 및 JSON 형식은 별도 파일로 저장해야 함\n",
    "for split, dataset in drug_dataset_clean.items():\n",
    "    dataset.to_json(f\"drug-reviews-{split}.jsonl\")\n",
    "\n",
    "# JSON Lines 형식으로 저장\n",
    "# !head -n 1 drug-reviews-train.jsonl\n",
    "\n",
    "# JSON 파일 로드\n",
    "data_files = {\n",
    "    \"train\": \"drug-reviews-train.jsonl\",\n",
    "    \"validation\": \"drug-reviews-validation.jsonl\",\n",
    "    \"test\": \"drug-reviews-test.jsonl\",\n",
    "}\n",
    "drug_dataset_reloaded = load_dataset(\"json\", data_files=data_files)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FAISS를 이용한 시맨틱 검색  \n",
    "- 시맨틱 검색을 위한 임베딩 사용하기\n",
    "  - 개별 임베딩을 pooling하여 전체 문장, 단락 또는 문서에 대한 벡터 표현을 생성할 수 있음\n",
    "  - 이런 임베딩을 사용하여 각 임베딩 사이의 내적 유사도 또는 다른 유사도 메트릭(similarity metric)을 계산하고 가장 많이 겹치는 문서를 반환하여 코퍼스에서 유사 문서 검색을 수행할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-eaac67148cd5cd4f/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'draft', 'pull_request', 'body', 'reactions', 'timeline_url', 'performed_via_github_app', 'is_pull_request'],\n",
      "    num_rows: 3651\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 로딩 및 준비 작업\n",
    "from huggingface_hub import hf_hub_url\n",
    "\n",
    "data_files = hf_hub_url(\n",
    "    repo_id=\"spasis/datasets-github-issues\",\n",
    "    filename=\"datasets-issues-with-comments.jsonl\",\n",
    "    repo_type=\"dataset\",)\n",
    "\n",
    "from datasets import load_dataset\n",
    "issues_dataset = load_dataset('json', data_files=data_files, split='train')\n",
    "print(issues_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 첫 번째는 pull requests를 필터링하는 것\n",
    "- Pull requests는 사용자 query에 응답하는데 거의 사용되지 않고 검색 엔진에 노이즈를 발생시키기 때문\n",
    "- Dataset.filter() 함수를 사용하여 데이터셋에서 이런 row을 제거할 수 있음\n",
    "  - 사용자 쿼리에 대한 답변을 제공할 수 없는 주석이 없는 행들도 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-eaac67148cd5cd4f/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-045281b01ed96bfe.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'draft', 'pull_request', 'body', 'reactions', 'timeline_url', 'performed_via_github_app', 'is_pull_request'],\n",
      "    num_rows: 997\n",
      "})\n",
      "Dataset({\n",
      "    features: ['html_url', 'title', 'comments', 'body'],\n",
      "    num_rows: 997\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "issues_dataset = issues_dataset.filter(\n",
    "    lambda x: (x['is_pull_request'] == False and len(x['comments']) > 0))\n",
    "print(issues_dataset)\n",
    "\n",
    "# 데이터셋에 많은 columns 이 있고 이들 대부분은 검색 대상이 아님\n",
    "# 검색 관점에서 가장 유익한 열은 title, body 및 comments 이며, \n",
    "# html_url은 소스 문제에 대한 링크 제공\n",
    "# Dataset.remove_columns()를 사용하여 나머지 열을 삭제\n",
    "columns = issues_dataset.column_names\n",
    "columns_to_keep = ['title', 'body', 'html_url', 'comments']\n",
    "columns_to_remove = set(columns_to_keep).symmetric_difference(columns)\n",
    "issues_dataset = issues_dataset.remove_columns(columns_to_remove)\n",
    "print(issues_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 임베딩을 생성하기 위해 각 이슈의 comments에 해당 이슈의 title과 body 내용을 추가\n",
    "  - 이러한 필드들에는 종종 유용한 컨텍스트 정보가 포함되기 때문\n",
    "  - 현재 데이터셋의 comments 열(column)은 현재 각 이슈에 대한 여러 개의 comments 리스트이므로 각 행이 (html_url, title, body, comment) 튜플로 구성되도록 해당 열을 분해해야 함\n",
    "  - explode : 단일 행의 특정 필드가 여러 항목을 포함하는 경우 여러 개의 행으로 확장하는 기법\n",
    "- DataFrame.explode() 함수는 리스트 형태로 구성된 열(list-like column)의 각 요소에 대해 새로운 행을 생성하고 해당 열이 아닌 다른 모든 열 값을 최초 행의 값으로 복제함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "                                            html_url  \\\n",
      "5  https://github.com/huggingface/datasets/issues...   \n",
      "6  https://github.com/huggingface/datasets/issues...   \n",
      "7  https://github.com/huggingface/datasets/issues...   \n",
      "8  https://github.com/huggingface/datasets/issues...   \n",
      "9  https://github.com/huggingface/datasets/issues...   \n",
      "\n",
      "                                               title  \\\n",
      "5    set_format(\"np\") no longer works for Image data   \n",
      "6    set_format(\"np\") no longer works for Image data   \n",
      "7    set_format(\"np\") no longer works for Image data   \n",
      "8    set_format(\"np\") no longer works for Image data   \n",
      "9  Datasets created with `push_to_hub` can't be a...   \n",
      "\n",
      "                                            comments  \\\n",
      "5  A quick fix for now is doing this:\\r\\n\\r\\n```p...   \n",
      "6  This error also propagates to jax and is even ...   \n",
      "7  Hi! We've recently introduced a new Image feat...   \n",
      "8  Yes I agree it should return arrays and not a ...   \n",
      "9  Thanks for reporting. I think this can be fixe...   \n",
      "\n",
      "                                                body  \n",
      "5  ## Describe the bug\\r\\n`dataset.set_format(\"np...  \n",
      "6  ## Describe the bug\\r\\n`dataset.set_format(\"np...  \n",
      "7  ## Describe the bug\\r\\n`dataset.set_format(\"np...  \n",
      "8  ## Describe the bug\\r\\n`dataset.set_format(\"np...  \n",
      "9  ## Describe the bug\\r\\nIn offline mode, one ca...  \n",
      "Dataset({\n",
      "    features: ['html_url', 'title', 'comments', 'body'],\n",
      "    num_rows: 3500\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "issues_dataset.set_format('pandas')\n",
    "df = issues_dataset[:]\n",
    "\n",
    "# 5번째 행은 해당 이슈와 관련된 4개의 comments가 있음\n",
    "print(len(df['comments'][5].tolist()))\n",
    "\n",
    "comments_df = df.explode('comments', ignore_index=True)\n",
    "# 개별 comment가 포함된 comments열과 함께 행이 복제됨\n",
    "print(comments_df[5:10])\n",
    "\n",
    "from datasets import Dataset\n",
    "comments_dataset = Dataset.from_pandas(comments_df)\n",
    "print(comments_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bc373215c38482f9fb40261eff17f74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cdb0683235a4a488571949be99b4afa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['html_url', 'title', 'comments', 'body', 'comment_length'],\n",
      "    num_rows: 2571\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b43337be97c4ccfa762ff904cfb3d8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2571 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# comment당 단어수를 저장하는 새로운 컬럼 생성\n",
    "comments_dataset = comments_dataset.map(\n",
    "    lambda x: {\"comment_length\": len(x['comments'].split())})\n",
    "# 짧은 댓글 필터링\n",
    "comments_dataset = comments_dataset.filter(\n",
    "    lambda x: x['comment_length'] > 15 and x['body'] is not None)\n",
    "print(comments_dataset)\n",
    "\n",
    "def concatenate_text(examples):\n",
    "    return {\n",
    "        'text': examples['title'] + ' \\n' + examples['body'] + ' \\n'\n",
    "        + examples['comments']}\n",
    "comments_dataset = comments_dataset.map(concatenate_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트 임베딩 생성  \n",
    "- AutoModel 클래스를 사용하여 토큰 임베딩을 얻음. 다음은 모델을 로드할 적절한 체크포인트(checkpoint)를 선택하는 것\n",
    "- 임베딩을 만드는데 사용할 sentence-transformers라는 라이브러리가 있음\n",
    "  - 라이브러리 문서에 설명된 대로 현재 만들고자 하는 코드는 비대칭 의미 검색(asymmetric semantic search) 의 한 예임\n",
    "  - 이슈 comments과 같이 더 긴 문서에서 답을 찾고자 하는 짧은 쿼리가 있기 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동일한 체크포인트를 사용하여 토크나이저 로드\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_ckpt = 'sentence-transformers/multi-qa-mpnet-base-dot-v1'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPNetModel(\n",
       "  (embeddings): MPNetEmbeddings(\n",
       "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): MPNetEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (relative_attention_bias): Embedding(32, 12)\n",
       "  )\n",
       "  (pooler): MPNetPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd062b5335234408bf721d8e11700df1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2571 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GitHub 이슈 말뭉치의 각 항목을 단일 벡터로 표현하고 싶기 때문에 어떤 방식으로든 \n",
    "# 토큰 임베딩을 pooling하거나 평균화(average)해야 함\n",
    "# 자주 사용하는 접근 방식은 모델의 출력에 대해 [CLS] pooling을 수행하는 것\n",
    "# 특수 [CLS] 토큰에 대한 last_hidden_state를 수집하면 됨\n",
    "\n",
    "def cls_pooling(model_output):\n",
    "    return model_output.last_hidden_state[:, 0]\n",
    "\n",
    "# 문서 토큰화 -> gpu에 텐서 배치 -> 모델에 공급 -> 출력에 CLS pooling 적용\n",
    "def get_embeddings(text_list):\n",
    "    encoded_input = tokenizer(\n",
    "        text_list, padding=True, truncation=True, return_tensors='pt')\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "    model_output = model(**encoded_input)\n",
    "    return cls_pooling(model_output)\n",
    "\n",
    "embedding = get_embeddings(comments_dataset['text'][0])\n",
    "print(embedding.shape)\n",
    "\n",
    "# dataset.map을 이용하여 새로운 임베딩 열 생성\n",
    "embeddings_dataset = comments_dataset.map(\n",
    "    # FAISS로 데이터셋을 인덱싱할 때 datasets에 numpy 타입이 필요함\n",
    "    lambda x: {\n",
    "        'embeddings': get_embeddings(x['text']).detach().cpu().numpy()[0]})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "효율적인 시맨틱 검색을 위한 FAISS 사용  \n",
    "- 임베딩 데이터셋이 있으므로 이를 검색할 방법이 필요함. 이를 위해 FAISS 인덱스 라고 하는 Datasets 내에서의 특별한 자료 구조를 사용함\n",
    "- FAISS(Facebook AI Similarity Search)는 임베딩 벡터를 빠르게 검색하고 클러스터링하는 효율적인 알고리즘을 제공하는 라이브러리\n",
    "- FAISS의 기본 아이디어는 입력 임베딩과 유사한 임베딩을 찾을 수 있는 인덱스(index) 라는 특수 데이터 구조를 만드는 것\n",
    "- Datasets에서 FAISS 인덱스를 만드는 것은 Dataset.add_faiss_index() 함수를 사용하고 인덱스할 데이터셋의 열을 지정함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "647a19ea628a4cd1acc1d8660a66d75b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 768)\n",
      "COMMENT: Requiring online connection is a deal breaker in some cases unfortunately so it'd be great if offline mode is added similar to how `transformers` loads models offline fine.\n",
      "\n",
      "@mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?\n",
      "SCORE: 25.505016326904297\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n",
      "COMMENT: The local dataset builders (csv, text , json and pandas) are now part of the `datasets` package since #1726 :)\n",
      "You can now use them offline\n",
      "```python\n",
      "datasets = load_dataset('text', data_files=data_files)\n",
      "```\n",
      "\n",
      "We'll do a new release soon\n",
      "SCORE: 24.5555419921875\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n",
      "COMMENT: I opened a PR that allows to reload modules that have already been loaded once even if there's no internet.\n",
      "\n",
      "Let me know if you know other ways that can make the offline mode experience better. I'd be happy to add them :) \n",
      "\n",
      "I already note the \"freeze\" modules option, to prevent local modules updates. It would be a cool feature.\n",
      "\n",
      "----------\n",
      "\n",
      "> @mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?\n",
      "\n",
      "Indeed `load_dataset` allows to load remote dataset script (squad, glue, etc.) but also you own local ones.\n",
      "For example if you have a dataset script at `./my_dataset/my_dataset.py` then you can do\n",
      "```python\n",
      "load_dataset(\"./my_dataset\")\n",
      "```\n",
      "and the dataset script will generate your dataset once and for all.\n",
      "\n",
      "----------\n",
      "\n",
      "About I'm looking into having `csv`, `json`, `text`, `pandas` dataset builders already included in the `datasets` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded.\n",
      "cf #1724 \n",
      "SCORE: 24.14898681640625\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n",
      "COMMENT: > here is my way to load a dataset offline, but it **requires** an online machine\n",
      "> \n",
      "> 1. (online machine)\n",
      "> \n",
      "> ```\n",
      "> \n",
      "> import datasets\n",
      "> \n",
      "> data = datasets.load_dataset(...)\n",
      "> \n",
      "> data.save_to_disk(/YOUR/DATASET/DIR)\n",
      "> \n",
      "> ```\n",
      "> \n",
      "> 2. copy the dir from online to the offline machine\n",
      "> \n",
      "> 3. (offline machine)\n",
      "> \n",
      "> ```\n",
      "> \n",
      "> import datasets\n",
      "> \n",
      "> data = datasets.load_from_disk(/SAVED/DATA/DIR)\n",
      "> \n",
      "> ```\n",
      "> \n",
      "> \n",
      "> \n",
      "> HTH.\n",
      "\n",
      "\n",
      "SCORE: 22.894001007080078\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n",
      "COMMENT: here is my way to load a dataset offline, but it **requires** an online machine\n",
      "1. (online machine)\n",
      "```\n",
      "import datasets\n",
      "data = datasets.load_dataset(...)\n",
      "data.save_to_disk(/YOUR/DATASET/DIR)\n",
      "```\n",
      "2. copy the dir from online to the offline machine\n",
      "3. (offline machine)\n",
      "```\n",
      "import datasets\n",
      "data = datasets.load_from_disk(/SAVED/DATA/DIR)\n",
      "```\n",
      "\n",
      "HTH.\n",
      "SCORE: 22.406658172607422\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings_dataset.add_faiss_index(column=\"embeddings\")\n",
    "\n",
    "# Dataset.get_nearest_examples() 함수로 근접 이웃 검색(nearest neighbor lookup)을\n",
    "# 수행하여 이 인덱스에 대한 쿼리를 수행할 수 있음\n",
    "# 먼저 질문을 삽입하여 테스트\n",
    "question = \"How can I load a dataset offline?\"\n",
    "question_embedding = get_embeddings([question]).cpu().detach().numpy()\n",
    "print(question_embedding.shape)\n",
    "\n",
    "# 해당 쿼리를 나타내는 768차원 벡터를 만들고 전체 코퍼스와 비교하여 \n",
    "# 가장 유사한 임베딩을 찾음\n",
    "scores, samples = embeddings_dataset.get_nearest_examples(\n",
    "    \"embeddings\", question_embedding, k=5)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Dataset.get_nearest_examples() 함수는 쿼리와 문서 간의 중첩(유사도) 순위를 \n",
    "# 매기는 점수 튜플(tuple)과 해당 샘플 집합(여기서는 가장 잘 일치하는 5개)을 반환\n",
    "samples_df = pd.DataFrame.from_dict(samples)\n",
    "samples_df[\"scores\"] = scores\n",
    "samples_df.sort_values(\"scores\", ascending=False, inplace=True)\n",
    "\n",
    "# 쿼리가 검색된 comments와 얼마나 잘 일치하는지 확인\n",
    "for _, row in samples_df.iterrows():\n",
    "    print(f\"COMMENT: {row.comments}\")\n",
    "    print(f\"SCORE: {row.scores}\")\n",
    "    print(f\"TITLE: {row.title}\")\n",
    "    print(f\"URL: {row.html_url}\")\n",
    "    print(\"=\" * 50)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
