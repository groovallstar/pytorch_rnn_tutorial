{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token='')\n",
    "\n",
    "# get a token: https://huggingface.co/docs/api-inference/quicktour#get-your-api-token\n",
    "from getpass import getpass\n",
    "HUGGINGFACEHUB_API_TOKEN = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# template = \"\"\"A chat between a curious user and an artificial intelligence assistant. \"\"\"\\\n",
    "#            \"\"\"The assistant gives helpful, detailed, and polite answers to the user's questions.\\nHuman: {question}\\nAssistant:\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Upstage/SOLAR-10.7B-v1.0\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Upstage/SOLAR-10.7B-v1.0\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"한국의 수도는 어디인가요? \"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=64)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "#model_id = 'beomi/Llama-3-Open-Ko-8B-Instruct-preview'\n",
    "#model_id = 'yanolja/EEVE-Korean-2.8B-v1.0'\n",
    "model_id = 'yanolja/EEVE-Korean-10.8B-v1.0'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = '''당신은 유용한 AI 어시스턴트입니다. 사용자의 질의에 대해 친절하고 정확하게 답변해야 합니다.'''\n",
    "instruction = \"다음 제목의 논문을 요약해줘 'Optimizing Language Augmentation for Multilingual Large Language Models: A Case Study on Korean'\"\n",
    "\n",
    "PROMPT = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\\nHuman: {prompt}\\nAssistant:\\n\"\n",
    "instruction = '한국의 수도는 어디인가요? 아래 선택지 중 골라주세요.\\n\\n(A) 경성\\n(B) 부산\\n(C) 평양\\n(D) 서울\\n(E) 전주'\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": f\"{PROMPT}\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{instruction}\"}\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=512,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=1,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama\n",
    "\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "# download model\n",
    "model_name_or_path = \"MLP-KTLim/llama-3-Korean-Bllossom-8B-gguf-Q4_K_M\" # repo id\n",
    "# 4bit\n",
    "model_basename = \"llama-3-Korean-Bllossom-8B-Q4_K_M.gguf\" # file name\n",
    "\n",
    "# model_name_or_path = \"teddylee777/Llama-3-Open-Ko-8B-gguf\" # repo id\n",
    "# model_basename = \"Llama-3-Open-Ko-8B-Q8_0.gguf\" # file name\n",
    "\n",
    "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n",
    "#print(model_path)\n",
    "\n",
    "# # CPU\n",
    "# lcpp_llm = Llama(\n",
    "#     model_path=model_path,\n",
    "#     n_threads=-1,\n",
    "#     )\n",
    "\n",
    "# GPU에서 사용하려면 아래 코드로 실행\n",
    "lcpp_llm = Llama(\n",
    "    model_path=model_path,\n",
    "    device=1,\n",
    "    n_threads=20, # CPU cores\n",
    "    n_batch=512, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "    n_gpu_layers=-1, # Change this value based on your model and your GPU VRAM pool.\n",
    "    n_ctx=4096, # Context window\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\\nHuman: {prompt}\\nAssistant:\\n\"\n",
    "text = '한국의 수도는 어디인가요? 아래 선택지 중 골라주세요.\\n\\n(A) 경성\\n(B) 부산\\n(C) 평양\\n(D) 서울\\n(E) 전주'\n",
    "\n",
    "prompt = prompt_template.format(prompt=text)\n",
    "\n",
    "start = time.time()\n",
    "response = lcpp_llm(\n",
    "    prompt=prompt,\n",
    "    max_tokens=256,\n",
    "    temperature=0.5,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    stop = ['</s>'], # Dynamic stopping when such token is detected.\n",
    "    echo=True # return the prompt\n",
    ")\n",
    "pprint(response)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who won the FIFA World Cup in the year 1994? \"\n",
    "template = \"\"\"Question: {question}\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "# HuggingFace Model ID\n",
    "#model_id = 'yanolja/EEVE-Korean-10.8B-v1.0'\n",
    "\n",
    "# download model\n",
    "model_name_or_path = \"heegyu/EEVE-Korean-Instruct-10.8B-v1.0-GGUF\" # repo id\n",
    "# 4bit\n",
    "model_id = \"yanolja/EEVE-Korean-2.8B-v1.0\" # file name\n",
    "\n",
    "# HuggingFacePipeline 객체 생성\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=model_id, \n",
    "    device=-1,               # -1: CPU(default), 0번 부터는 CUDA 디바이스 번호 지정시 GPU 사용하여 추론\n",
    "    task=\"text-generation\", # 텍스트 생성\n",
    "    model_kwargs={\"temperature\": 0.1, \n",
    "                  \"max_length\": 64},\n",
    ")\n",
    "\n",
    "# 템플릿\n",
    "template = \"\"\"질문: {question}\n",
    "\n",
    "답변: \"\"\"\n",
    "\n",
    "# 프롬프트 템플릿 생성\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# LLM Chain 객체 생성\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"대한민국의 수도?\"\n",
    "print(llm_chain.run(question=question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama\n",
    "\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "# download model\n",
    "model_name_or_path = \"heegyu/EEVE-Korean-Instruct-10.8B-v1.0-GGUF\" # repo id\n",
    "# 4bit\n",
    "model_basename = \"ggml-model-Q4_K_M.gguf\" # file name\n",
    "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n",
    "#print(model_path)\n",
    "\n",
    "# CPU\n",
    "# lcpp_llm = Llama(\n",
    "#     model_path=model_path,\n",
    "#     n_threads=2,\n",
    "#     )\n",
    "\n",
    "# GPU에서 사용하려면 아래 코드로 실행\n",
    "lcpp_llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_threads=2, # CPU cores\n",
    "    n_batch=512, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "    n_gpu_layers=-1, # Change this value based on your model and your GPU VRAM pool.\n",
    "    n_ctx=4096, # Context window\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "\n",
    "prompt_template = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\\nHuman: {prompt}\\nAssistant:\\n\"\n",
    "text = '한국의 수도는 어디인가요? 아래 선택지 중 골라주세요.\\n\\n(A) 경성\\n(B) 부산\\n(C) 평양\\n(D) 서울\\n(E) 전주'\n",
    "\n",
    "prompt = prompt_template.format(prompt=text)\n",
    "\n",
    "start = time.time()\n",
    "response = lcpp_llm(\n",
    "    prompt=prompt,\n",
    "    max_tokens=256,\n",
    "    temperature=0.5,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    stop = ['</s>'], # Dynamic stopping when such token is detected.\n",
    "    echo=True # return the prompt\n",
    ")\n",
    "pprint(response)\n",
    "print(time.time() - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\\nHuman: {prompt}\\nAssistant:\\n\"\n",
    "text = '한국의 수도는 어디인가요? 아래 선택지 중 골라주세요.\\n\\n(A) 경성\\n(B) 부산\\n(C) 평양\\n(D) 서울\\n(E) 전주'\n",
    "\n",
    "prompt = prompt_template.format(prompt=text)\n",
    "\n",
    "start = time.time()\n",
    "response = lcpp_llm(\n",
    "    prompt=prompt,\n",
    "    max_tokens=256,\n",
    "    temperature=0.5,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    stop = ['</s>'], # Dynamic stopping when such token is detected.\n",
    "    echo=True # return the prompt\n",
    ")\n",
    "pprint(response)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set CUDA_DOCKER_ARCH=all\n",
    "set LLAMA_CUBLAS=1\n",
    "set CMAKE_ARGS=-DLLAMA_CUBLAS=on\n",
    "set FORCE_CMAKE=1\n",
    "pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !CMAKE_ARGS=\"-DLLAMA_CUDA=on\" pip install llama-cpp-python\n",
    "# !huggingface-cli download MLP-KTLim/llama-3-Korean-Bllossom-8B-gguf-Q4_K_M --local-dir='~/.cache/huggingface/hub/'\n",
    "\n",
    "from llama_cpp import Llama\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = 'MLP-KTLim/llama-3-Korean-Bllossom-8B-gguf-Q4_K_M'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = Llama(\n",
    "    model_path='~/.cache/huggingface/hub/llama-3-Korean-Bllossom-8B-Q4_K_M.gguf',\n",
    "    n_ctx=512,\n",
    "    n_gpu_layers=-1        # Number of model layers to offload to GPU\n",
    ")\n",
    "\n",
    "PROMPT = \\\n",
    "'''당신은 유용한 AI 어시스턴트입니다. 사용자의 질의에 대해 친절하고 정확하게 답변해야 합니다.\n",
    "You are a helpful AI assistant, you'll need to answer users' queries in a friendly and accurate manner.'''\n",
    "\n",
    "instruction = 'Your Instruction'\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": f\"{PROMPT}\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{instruction}\"}\n",
    "    ]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    tokenize = False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"max_tokens\":512,\n",
    "    \"stop\":[\"<|eot_id|>\"],\n",
    "    \"top_p\":0.9,\n",
    "    \"temperature\":0.6,\n",
    "    \"echo\":True, # Echo the prompt in the output\n",
    "}\n",
    "\n",
    "resonse_msg = model(prompt, **generation_kwargs)\n",
    "print(resonse_msg['choices'][0]['text'][len(prompt):])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\\nHuman: {prompt}\\nAssistant:\\n\"\n",
    "text = '한국의 수도는 어디인가요? 아래 선택지 중 골라주세요.\\n\\n(A) 경성\\n(B) 부산\\n(C) 평양\\n(D) 서울\\n(E) 전주'\n",
    "\n",
    "prompt = prompt_template.format(prompt=text)\n",
    "\n",
    "start = time.time()\n",
    "response = lcpp_llm(\n",
    "    prompt=prompt,\n",
    "    max_tokens=256,\n",
    "    temperature=0.5,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    stop = ['</s>'], # Dynamic stopping when such token is detected.\n",
    "    echo=True # return the prompt\n",
    ")\n",
    "pprint(response)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"A chat between a curious user and an artificial intelligence assistant. \"\"\"\\\n",
    "           \"\"\"The assistant gives helpful, detailed, and polite answers to the user's questions.\\n Use only Korean.\\n\"\"\"\\\n",
    "           \"\"\"Human: {question}\\nAssistant:\\n\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "n_gpu_layers = -1  # The number of layers to put on the GPU. The rest will be on the CPU. If you don't know how many layers there are, you can use -1 to move all to GPU.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"~/.huggingface/hub/models--heegyu--EEVE-Korean-Instruct-10.8B-v1.0-GGUF/snapshots/9bf4892cf2017362dbadf99bd9a3523387135362/ggml-model-Q4_K_M.gguf\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    max_tokens=512,\n",
    "    temperature=0.5,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=False,  # Verbose is required to pass to the callback manager\n",
    ")\n",
    "\n",
    "llm_chain = prompt | llm\n",
    "question = \"\"\"사과 한 박스에는 사과가 30개 들어있는데, 처음에는 사과 3박스가 있었고, 내가 사과 5개를 먹었어. 남은 사과는 총 몇개야?\"\"\"\n",
    "llm_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "n_gpu_layers = -1  # The number of layers to put on the GPU. The rest will be on the CPU. If you don't know how many layers there are, you can use -1 to move all to GPU.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/work/.huggingface/hub/models--heegyu--EEVE-Korean-Instruct-10.8B-v1.0-GGUF/snapshots/9bf4892cf2017362dbadf99bd9a3523387135362/ggml-model-Q4_K_M.gguf\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    max_tokens=1024,\n",
    "    temperature=1.0,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    stop=['</s>'],\n",
    "    #callback_manager=callback_manager,\n",
    "    verbose=False,  # Verbose is required to pass to the callback manager\n",
    ")\n",
    "\n",
    "question = \"\"\"사과 한 박스에는 사과가 30개 들어있는데, 처음에는 사과 3박스가 있었고, 내가 사과 5개를 먹었어. 남은 사과는 총 몇개야?\"\"\"\n",
    "\n",
    "print(f'[답변]: {llm.invoke(question)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !huggingface-cli download MLP-KTLim/llama-3-Korean-Bllossom-8B-gguf-Q4_K_M\n",
    "\n",
    "from llama_cpp import Llama\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = 'MLP-KTLim/llama-3-Korean-Bllossom-8B-gguf-Q4_K_M'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = Llama(\n",
    "    model_path='/work/.huggingface/hub/models--MLP-KTLim--llama-3-Korean-Bllossom-8B-gguf-Q4_K_M/snapshots/a2084328644c0b4438a9ac1935cbb34d7801af7c/llama-3-Korean-Bllossom-8B-Q4_K_M.gguf',\n",
    "    #n_batch=512,\n",
    "    n_gpu_layers=-1        # Number of model layers to offload to GPU\n",
    ")\n",
    "\n",
    "PROMPT = \\\n",
    "'''당신은 유용한 AI 어시스턴트입니다. 사용자의 질의에 대해 친절하고 정확하게 답변해야 합니다.\n",
    "You are a helpful AI assistant, you'll need to answer users' queries in a friendly and accurate manner.'''\n",
    "\n",
    "instruction = '''사과 한 박스에는 사과가 30개 들어있는데, 처음에는 사과 3박스가 있었고, 내가 사과 5개를 먹었어. 남은 사과는 총 몇개야?'''\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": f\"{PROMPT}\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{instruction}\"}\n",
    "    ]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    tokenize = False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"max_tokens\":2048,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\":0.9,\n",
    "    \"temperature\":0.9,\n",
    "    \"stop\":[\"<|eot_id|>\"],\n",
    "    \"echo\":True, # Echo the prompt in the output\n",
    "}\n",
    "\n",
    "resonse_msg = model(prompt, **generation_kwargs)\n",
    "print(resonse_msg['choices'][0]['text'][len(prompt):])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스기사 내용을 로드하고, 청크로 나누고, 인덱싱합니다.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://n.news.naver.com/mnews/article/016/0002315162\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            \"div\",\n",
    "            attrs={\"class\": [\"newsct_article _article_body\",\n",
    "                             \"media_end_head_title\"]},\n",
    "        )),)\n",
    "docs = loader.load()\n",
    "print(f\"문서의 수: {len(docs)}\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터스토어를 생성합니다.\n",
    "# 임베딩 모델 로드\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"jhgan/ko-sroberta-multitask\", encode_kwargs={'normalize_embeddings': True})\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n",
    "\n",
    "# 뉴스에 포함되어 있는 정보를 검색하고 생성합니다.\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    # 검색한 문서 결과를 하나의 문단으로 합쳐줍니다.\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"A chat between a curious user and an artificial intelligence assistant. \"\"\"\\\n",
    "           \"\"\"The assistant gives helpful, detailed, and polite answers to the user's questions.\\n\"\"\"\\\n",
    "           \"\"\"Human: {question}\\nAssistant:\\n\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "n_gpu_layers = -1  # The number of layers to put on the GPU. The rest will be on the CPU. If you don't know how many layers there are, you can use -1 to move all to GPU.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/work/.huggingface/hub/models--heegyu--EEVE-Korean-Instruct-10.8B-v1.0-GGUF/snapshots/9bf4892cf2017362dbadf99bd9a3523387135362/ggml-model-Q4_K_M.gguf\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    max_tokens=1024,\n",
    "    temperature=0.1,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    stop=['</s>'],\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=False,  # Verbose is required to pass to the callback manager\n",
    ")\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"SH공사에서 방화 스카이포레라는 상표에 대해 알려줘.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke(\"LH에서 창원에 공급하는 브랜드 이름이 무엇인가요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers==4.41.1\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"CohereForAI/aya-23-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# Format message with the command-r-plus chat template\n",
    "messages = [{\"role\": \"user\", \"content\": \"Anneme onu ne kadar sevdiğimi anlatan bir mektup yaz\"}]\n",
    "input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "## <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Anneme onu ne kadar sevdiğimi anlatan bir mektup yaz<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n",
    "\n",
    "gen_tokens = model.generate(\n",
    "    input_ids, \n",
    "    max_new_tokens=100, \n",
    "    do_sample=True, \n",
    "    temperature=0.3,\n",
    "    )\n",
    "\n",
    "gen_text = tokenizer.decode(gen_tokens[0])\n",
    "print(gen_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "n_gpu_layers = -1  # The number of layers to put on the GPU. The rest will be on the CPU. If you don't know how many layers there are, you can use -1 to move all to GPU.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "model = LlamaCpp(\n",
    "    model_path=\"/work/.huggingface/hub/models--heegyu--EEVE-Korean-Instruct-10.8B-v1.0-GGUF/snapshots/9bf4892cf2017362dbadf99bd9a3523387135362/ggml-model-Q4_K_M.gguf\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    max_tokens=2048,\n",
    "    temperature=0.5,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    callback_manager=callback_manager,\n",
    "    stop=['</s>'],\n",
    "    echo=True,\n",
    "    verbose=False,  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# template 정의\n",
    "template = \"\"\"\n",
    "당신은 친절하게 답변해 주는 친절 봇입니다. 사용자의 질문에 [FORMAT]에 맞추어 답변해 주세요.\n",
    "답변은 항상 한글로 작성해 주세요.\n",
    "\n",
    "질문:\n",
    "{question}에 대하여 설명해 주세요.\n",
    "\n",
    "FORMAT:\n",
    "- 개요:\n",
    "- 예시:\n",
    "- 출처:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_template.format(country='대한민국')\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output \n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 나라에 대하여 수도를 묻는 프롬프트 템플릿을 생성합니다.\n",
    "# prompt = PromptTemplate.from_template(\"{country}의 수도는 어디인가요?\")\n",
    "\n",
    "# 주어진 나라에 대하여 수도를 묻는 프롬프트 템플릿을 생성합니다.\n",
    "template = \"\"\"\n",
    "당신은 친절하게 답변해 주는 친절 봇입니다. 사용자의 질문에 [FORMAT]에 맞추어 답변해 주세요.\n",
    "답변은 항상 한글로 작성해 주세요.\n",
    "\n",
    "질문:\n",
    "{question}에 대하여 설명해 주세요.\n",
    "\n",
    "FORMAT:\n",
    "- 개요:\n",
    "- 예시:\n",
    "- 출처:\n",
    "\"\"\"\n",
    "\n",
    "# template = \"\"\"\n",
    "# 당신은 영어를 가르치는 10년차 영어 선생님입니다. 상황에 [FORMAT]에 영어 회화를 작성해 주세요.\n",
    "\n",
    "# 상황:\n",
    "# {question}\n",
    "\n",
    "# FORMAT:\n",
    "# - 영어 회화:\n",
    "# - 한글 해석:\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "# 문자열 출력 파서를 초기화합니다.\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# 프롬프트, 모델, 출력 파서를 연결하여 처리 체인을 구성합니다.\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "# 완성된 Chain 을 이용하여 country 를 '대한민국'으로 설정하여 실행합니다.\n",
    "# chain.invoke({\"country\": \"대한민국\"})\n",
    "print(chain.invoke({\"question\": \"저는 식당에 가서 음식을 주문하고 싶어요\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt 를 PromptTemplate 객체로 생성합니다.\n",
    "prompt = PromptTemplate.from_template(\"{topic} 에 대해 쉽게 설명해주세요.\")\n",
    "\n",
    "# input 딕셔너리에 주제를 'ice cream'으로 설정합니다.\n",
    "input = {\"topic\": \"양자역학\"}\n",
    "\n",
    "# prompt 객체의 invoke 메서드를 사용하여 input을 전달하고 대화형 프롬프트 값을 생성합니다.\n",
    "prompt.invoke(input)\n",
    "\n",
    "# prompt 객체와 model 객체를 파이프(|) 연산자로 연결하고 invoke 메서드를 사용하여 input을 전달합니다.\n",
    "# 이를 통해 AI 모델이 생성한 메시지를 반환합니다.\n",
    "(prompt | model).invoke(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runnable 프로토콜\n",
    "# stream: 응답의 청크를 스트리밍함\n",
    "# invoke: 입력에 대한 체인을 호출함\n",
    "# batch: 입력 목록에 대해 체인을 호출함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template('{topic} 에 대하여 3문장으로 설명해줘.')\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 스키마\n",
    "chain.input_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def print_schema(schema):\n",
    "    print(json.dumps(schema, indent=4))\n",
    "    \n",
    "input_schema = {\n",
    "    'type': 'object',\n",
    "    'properties': {'name': {'type': 'string'},\n",
    "                   'age': {'type': 'integer', 'minimum': 0},\n",
    "    'required': ['name', 'age'],}\n",
    "}\n",
    "\n",
    "print_schema(input_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unicode_str = \"\\u3010\\u697d\\u5929\\u5e02\\u5834\\u3011\\u30a2\\u30ab\\u30a6\\u30f3\\u30c8\\u306e\\u652f\\u6255\\u3044\\u65b9\\u6cd5\\u3092\\u78ba\\u8a8d\\u3067\\u304d\\u305a\\u3001\\u6ce8\\u6587\\u3092\\u30ad\\u30e3\\u30f3\\u30bb\\u30eb\\u3067\\u304d\\u307e\\u305b\\u3093.\"\n",
    "# actual_char = unicode_str.encode().decode('unicode_escape')\n",
    "# print(actual_char)\n",
    "unicode_str = \"\\u3010\\u697d\\u5929\\u5e02\\u5834\\u3011\\u30a2\\u30ab\\u30a6\\u30f3\\u30c8\\u306e\\u652f\\u6255\\u3044\\u65b9\\u6cd5\\u3092\\u78ba\\u8a8d\\u3067\\u304d\\u305a\\u3001\\u6ce8\\u6587\\u3092\\u30ad\\u30e3\\u30f3\\u30bb\\u30eb\\u3067\\u304d\\u307e\\u305b\\u3093.\"\n",
    "actual_char = bytes(unicode_str, 'utf-8').decode('unicode_escape')\n",
    "actual_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 입력 스키마를 출력합니다.\n",
    "model.input_schema.schema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체인의 출력 스키마는 마지막 부분의 출력 스키마입니다. 이 경우 ChatModel의 출력 스키마로, ChatMessage를 출력합니다.\n",
    "chain.output_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain.stream 메서드를 사용하여 '멀티모달' 토픽에 대한 스트림을 생성하고 반복합니다.\n",
    "for s in chain.stream({\"topic\": \"멀티모달\"}):\n",
    "    # 스트림에서 받은 데이터의 내용을 출력합니다. 줄바꿈 없이 이어서 출력하고, 버퍼를 즉시 비웁니다.\n",
    "    print(s.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain 객체의 invoke 메서드를 호출하고, 'ChatGPT'라는 주제로 딕셔너리를 전달합니다.\n",
    "chain.invoke({\"topic\": \"ChatGPT\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 토픽 리스트를 batch 처리하는 함수 호출\n",
    "chain.batch([{\"topic\": \"ChatGPT\"}, {\"topic\": \"Instagram\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.batch(\n",
    "    [\n",
    "        {\"topic\": \"ChatGPT\"},\n",
    "        {\"topic\": \"Instagram\"},\n",
    "        {\"topic\": \"멀티모달\"},\n",
    "        {\"topic\": \"프로그래밍\"},\n",
    "        {\"topic\": \"머신러닝\"},\n",
    "    ],\n",
    "    config={\"max_concurrency\": 3},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
